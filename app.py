from flask import Flask, jsonify, request, render_template, session, redirect, url_for
import requests
import datetime
from functools import wraps # V5.0
import pandas as pd
import numpy as np
import os
import uuid
import json
import time
import random

# V5.0: å¢å¼ºçš„ç¯å¢ƒé…ç½®
DATABASE_URL = os.environ.get('DATABASE_URL')
IS_PRODUCTION = bool(DATABASE_URL)

if IS_PRODUCTION:
    # ç”Ÿäº§ç¯å¢ƒ - ä½¿ç”¨PostgreSQL
    import psycopg2
    from psycopg2.extras import RealDictCursor
    USE_POSTGRESQL = True
    print("[INFO] ç”Ÿäº§ç¯å¢ƒ - ä½¿ç”¨PostgreSQLæ•°æ®åº“")
else:
    # å¼€å‘ç¯å¢ƒ - ä½¿ç”¨SQLite
    import sqlite3
    USE_POSTGRESQL = False
    DATABASE_PATH = 'annotations.db'
    print("[INFO] å¼€å‘ç¯å¢ƒ - ä½¿ç”¨SQLiteæ•°æ®åº“")

app = Flask(__name__, template_folder='templates', static_folder='static')

# ç¡®ä¿JSONå“åº”ç›´æ¥è¾“å‡ºUTF-8ä¸­æ–‡ï¼Œè€Œä¸æ˜¯\uXXXXè½¬ä¹‰
# ä»…å½±å“jsonifyçš„è¾“å‡ºï¼Œä¸æ”¹å˜å…¶ä»–é€»è¾‘
app.config['JSON_AS_ASCII'] = False

# V5.0: ä¼šè¯å’Œå®‰å…¨é…ç½®
# ä»ç¯å¢ƒå˜é‡åŠ è½½å¯†é’¥å’Œå¯†ç 
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key-for-local-testing')
APP_PASSWORD = os.environ.get('APP_PASSWORD', 'password') # æœ¬åœ°å¼€å‘çš„é»˜è®¤å¯†ç 

if IS_PRODUCTION and app.config['SECRET_KEY'] == 'dev-secret-key-for-local-testing':
    print("[WARNING] å®‰å…¨è­¦æŠ¥: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨é»˜è®¤çš„SECRET_KEYæ˜¯ä¸å®‰å…¨çš„ï¼")
if not IS_PRODUCTION:
    print(f"[INFO] å¼€å‘ç¯å¢ƒç™»å½•å¯†ç æ˜¯: {APP_PASSWORD}")

@app.before_request
def make_session_permanent():
    session.permanent = True
    app.permanent_session_lifetime = datetime.timedelta(days=7)

# V5.0: ç™»å½•é€»è¾‘
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # ä»…åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¼ºåˆ¶æ‰§è¡Œç™»å½•
        if IS_PRODUCTION and 'logged_in' not in session:
            # å°†ç”¨æˆ·é‡å®šå‘åˆ°ç™»å½•é¡µé¢ï¼Œå¹¶åœ¨URLä¸­é™„å¸¦ä»–ä»¬æƒ³è®¿é—®çš„é¡µé¢
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

# V5.8: æ–°å¢æ··åˆè®¤è¯ç³»ç»Ÿ - æ”¯æŒWeb Session + Basic Auth
def require_api_auth(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # åœ¨å¼€å‘ç¯å¢ƒä¸‹ï¼Œä¸å¼ºåˆ¶è®¤è¯ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
        if not IS_PRODUCTION:
            return f(*args, **kwargs)
        
        # ç”Ÿäº§ç¯å¢ƒä¸‹æ£€æŸ¥è®¤è¯
        # æ–¹å¼1: æ£€æŸ¥Web Sessionï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        if 'logged_in' in session:
            return f(*args, **kwargs)
        
        # æ–¹å¼2: æ£€æŸ¥Basic Authï¼ˆæ–°å¢APIå‹å¥½æ–¹å¼ï¼‰
        # æ³¨æ„ï¼šç”¨æˆ·åå›ºå®šä¸º'api'ï¼Œå¯†ç ä»ç¯å¢ƒå˜é‡è¯»å–
        auth = request.authorization
        if auth and auth.username == 'api' and APP_PASSWORD and auth.password == APP_PASSWORD:
            return f(*args, **kwargs)
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›401é”™è¯¯ï¼ˆAPIè°ƒç”¨ä¸é‡å®šå‘åˆ°ç™»å½•é¡µé¢ï¼‰
        return jsonify({
            'error': 'Authentication required',
            'message': 'Please provide authentication via Web login or Basic Auth (username: api, password: from env)'
        }), 401
    
    return decorated_function

# --- Database Setup ---
def get_db():
    if IS_PRODUCTION: # V5.0: ä½¿ç”¨IS_PRODUCTION
        # ç”Ÿäº§ç¯å¢ƒ - PostgreSQL
        conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
    else:
        # å¼€å‘ç¯å¢ƒ - SQLite
        conn = sqlite3.connect(DATABASE_PATH)
        conn.row_factory = sqlite3.Row  # è®©ç»“æœå¯ä»¥åƒå­—å…¸ä¸€æ ·è®¿é—®
    return conn

def db_execute(cursor, query, params=None):
    """æ™ºèƒ½æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ï¼Œè‡ªåŠ¨å¤„ç†SQLiteå’ŒPostgreSQLçš„å ä½ç¬¦å·®å¼‚"""
    # V5.0: ç§»é™¤æ—§çš„USE_POSTGRESQLæ£€æŸ¥ï¼Œé€»è¾‘ç®€åŒ–
    if not IS_PRODUCTION and query and '%s' in query:
        # SQLiteç¯å¢ƒï¼šå°†%så ä½ç¬¦æ›¿æ¢ä¸º?
        query = query.replace('%s', '?')
    
    if params:
        return cursor.execute(query, params)
    else:
        return cursor.execute(query)

def save_algorithm_annotation(ticker, date, text, algorithm_type, algorithm_params=None):
    """ä¿å­˜ç®—æ³•ç”Ÿæˆçš„æ³¨é‡Šåˆ°æ•°æ®åº“"""
    try:
        db = get_db()
        cursor = db.cursor()
        
        # ç”Ÿæˆå”¯ä¸€çš„æ³¨é‡ŠID
        annotation_id = f"algo-{ticker}-{date}-{algorithm_type}-{uuid.uuid4().hex[:8]}"
        
        # V4.8.1: å¢å¼ºé‡å¤æ£€æŸ¥é€»è¾‘ - ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨AIåˆ†æè®°å½•
        db_execute(cursor, """
            SELECT annotation_id, text, algorithm_type, is_deleted, is_favorite FROM annotations
            WHERE ticker = %s AND date = %s AND algorithm_type = 'ai_analysis' AND is_deleted = 0
        """, (ticker, date))
        
        ai_existing = cursor.fetchone()
        if ai_existing:
            # å¦‚æœå·²å­˜åœ¨AIåˆ†æè®°å½•ï¼Œä¸ç”Ÿæˆæ–°çš„ç®—æ³•è®°å½•ï¼ˆAIåˆ†æä¼˜å…ˆçº§æ›´é«˜ï¼‰
            print(f"[INFO] è·³è¿‡ç®—æ³•è®°å½•ç”Ÿæˆ {ticker}-{date}-{algorithm_type}ï¼šå·²å­˜åœ¨AIåˆ†æè®°å½• {ai_existing['annotation_id']}")
            cursor.close()
            db.close()
            return {'id': ai_existing['annotation_id'], 'text': ai_existing['text'], 'exists': True, 'type': 'ai_analysis', 'is_favorite': bool(ai_existing['is_favorite']) if ai_existing['is_favorite'] is not None else False}
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„ç®—æ³•æ³¨é‡Šï¼ˆåŒä¸€è‚¡ç¥¨ã€åŒä¸€æ—¥æœŸã€åŒä¸€ç®—æ³•ç±»å‹ï¼‰
        db_execute(cursor, """
            SELECT annotation_id, text, is_deleted, is_favorite FROM annotations
            WHERE ticker = %s AND date = %s AND algorithm_type = %s
        """, (ticker, date, algorithm_type))
        
        existing = cursor.fetchone()
        if existing:
            if existing['is_deleted'] == 0:
                # å¦‚æœå­˜åœ¨æœªåˆ é™¤çš„æ³¨é‡Šï¼Œè¿”å›ç°æœ‰æ³¨é‡Šçš„IDå’Œå†…å®¹
                print(f"[INFO] å¤ç”¨ç°æœ‰ç®—æ³•è®°å½•: {existing['annotation_id']}")
                cursor.close()
                db.close()
                return {'id': existing['annotation_id'], 'text': existing['text'], 'exists': True, 'is_favorite': bool(existing['is_favorite']) if existing['is_favorite'] is not None else False}
            else:
                # å¦‚æœå­˜åœ¨å·²åˆ é™¤çš„æ³¨é‡Šï¼Œä¸åˆ›å»ºæ–°æ³¨é‡Šï¼ˆä¿æŒåˆ é™¤çŠ¶æ€ï¼‰
                print(f"[INFO] è·³è¿‡å·²åˆ é™¤çš„è®°å½•: {ticker}-{date}-{algorithm_type}")
                cursor.close()
                db.close()
                return None
        
        # ä¿å­˜æ–°çš„ç®—æ³•æ³¨é‡Š
        params_json = json.dumps(algorithm_params) if algorithm_params else None
        db_execute(cursor, """
            INSERT INTO annotations
            (annotation_id, ticker, date, text, annotation_type, algorithm_type, algorithm_params, created_at, updated_at)
            VALUES (%s, %s, %s, %s, 'algorithm', %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        """, (annotation_id, ticker, date, text, algorithm_type, params_json))
        
        db.commit()
        cursor.close()
        db.close()
        
        print(f"[INFO] æ–°å»ºç®—æ³•è®°å½•: {annotation_id} - {text}")
        return {'id': annotation_id, 'text': text, 'exists': False, 'is_favorite': False}
        
    except Exception as e:
        print(f"[ERROR] ä¿å­˜ç®—æ³•æ³¨é‡Šå¤±è´¥: {e}")
        return None

def init_db():
    with app.app_context():
        conn = get_db()
        cursor = conn.cursor()
        
        if IS_PRODUCTION: # V5.0
            # PostgreSQL - æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'annotations'
                )
            """)
            result = cursor.fetchone()
            table_exists = result[0] if isinstance(result, (list, tuple)) else result['exists']
        else:
            # SQLite - æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='annotations'
            """)
            table_exists = cursor.fetchone() is not None
        
        if not table_exists:
            if IS_PRODUCTION: # V5.0
                # PostgreSQLè¯­æ³•
                cursor.execute('''
                    CREATE TABLE annotations (
                        id SERIAL PRIMARY KEY,
                        annotation_id TEXT NOT NULL UNIQUE,
                        ticker TEXT NOT NULL,
                        date TEXT NOT NULL,
                        text TEXT NOT NULL,
                        annotation_type TEXT NOT NULL DEFAULT 'manual',
                        algorithm_type TEXT,
                        algorithm_params TEXT,
                        original_text TEXT,
                        ai_analysis TEXT,
                        is_deleted INTEGER DEFAULT 0,
                        is_favorite INTEGER DEFAULT 0,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
            else:
                # SQLiteè¯­æ³•
                cursor.execute('''
                    CREATE TABLE annotations (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        annotation_id TEXT NOT NULL UNIQUE,
                        ticker TEXT NOT NULL,
                        date TEXT NOT NULL,
                        text TEXT NOT NULL,
                        annotation_type TEXT NOT NULL DEFAULT 'manual',
                        algorithm_type TEXT,
                        algorithm_params TEXT,
                        original_text TEXT,
                        ai_analysis TEXT,
                        is_deleted INTEGER DEFAULT 0,
                        is_favorite INTEGER DEFAULT 0,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        deleted_at TIMESTAMP NULL
                    )
            ''')
            print("âœ… åˆ›å»ºäº†annotationsè¡¨")
        else:
            print("ğŸ“‹ annotationsè¡¨å·²å­˜åœ¨")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ is_favoriteå­—æ®µ
            if IS_PRODUCTION: # V5.0
                # PostgreSQL - æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT column_name FROM information_schema.columns 
                    WHERE table_name = 'annotations' AND column_name = 'is_favorite'
                """)
                has_favorite_field = cursor.fetchone() is not None
            else:
                # SQLite - æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                cursor.execute("PRAGMA table_info(annotations)")
                columns = cursor.fetchall()
                has_favorite_field = any(col[1] == 'is_favorite' for col in columns)
            
            if not has_favorite_field:
                print("ğŸ”§ æ·»åŠ is_favoriteå­—æ®µ...")
                if IS_PRODUCTION: # V5.0
                    cursor.execute("ALTER TABLE annotations ADD COLUMN is_favorite INTEGER DEFAULT 0")
                else:
                    cursor.execute("ALTER TABLE annotations ADD COLUMN is_favorite INTEGER DEFAULT 0")
                print("âœ… is_favoriteå­—æ®µæ·»åŠ æˆåŠŸ")
            else:
                print("ğŸ“‹ is_favoriteå­—æ®µå·²å­˜åœ¨")
        
        # æ£€æŸ¥company_namesè¡¨æ˜¯å¦å·²å­˜åœ¨
        if IS_PRODUCTION: # V5.0
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = 'company_names'
                )
            """)
            result = cursor.fetchone()
            company_table_exists = result[0] if isinstance(result, (list, tuple)) else result['exists']
        else:
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='company_names'
            """)
            company_table_exists = cursor.fetchone() is not None
        
        if not company_table_exists:
            cursor.execute('''
                CREATE TABLE company_names (
                    ticker TEXT PRIMARY KEY,
                    company_name TEXT NOT NULL,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    source TEXT DEFAULT 'api',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            print("âœ… åˆ›å»ºäº†company_namesè¡¨")
            
            # å°†ç°æœ‰çš„æœ¬åœ°æ˜ å°„æ•°æ®æ’å…¥åˆ°æ•°æ®åº“ä¸­ (PostgreSQLè¯­æ³•)
            local_mappings = [
                ('ONC', 'ç™¾æµç¥å·', 'local'),
                ('6160.hk', 'ç™¾æµç¥å·', 'local'),
                ('6160.HK', 'ç™¾æµç¥å·', 'local'),
                ('BGNE', 'ç™¾æµç¥å·', 'local'),
                ('6855.hk', 'äºšç››åŒ»è¯', 'local'),
                ('6855.HK', 'äºšç››åŒ»è¯', 'local'),
                ('AAPL', 'è‹¹æœå…¬å¸', 'local'),
                ('TSLA', 'ç‰¹æ–¯æ‹‰', 'local'),
                ('MSFT', 'å¾®è½¯', 'local'),
                ('GOOGL', 'è°·æ­Œ', 'local'),
                ('AMZN', 'äºšé©¬é€Š', 'local'),
                ('NVDA', 'è‹±ä¼Ÿè¾¾', 'local'),
                ('META', 'Meta Platforms', 'local'),
                ('0700.hk', 'è…¾è®¯æ§è‚¡', 'local'),
                ('0700.HK', 'è…¾è®¯æ§è‚¡', 'local'),
                ('9988.hk', 'é˜¿é‡Œå·´å·´', 'local'),
                ('9988.HK', 'é˜¿é‡Œå·´å·´', 'local'),
                ('3690.hk', 'ç¾å›¢', 'local'),
                ('3690.HK', 'ç¾å›¢', 'local'),
                ('2318.hk', 'ä¸­å›½å¹³å®‰', 'local'),
                ('2318.HK', 'ä¸­å›½å¹³å®‰', 'local'),
                ('0941.hk', 'ä¸­å›½ç§»åŠ¨', 'local'),
                ('0941.HK', 'ä¸­å›½ç§»åŠ¨', 'local'),
                ('1810.hk', 'å°ç±³é›†å›¢', 'local'),
                ('1810.HK', 'å°ç±³é›†å›¢', 'local'),
                ('9999.hk', 'ç½‘æ˜“', 'local'),
                ('9999.HK', 'ç½‘æ˜“', 'local'),
                ('0388.hk', 'é¦™æ¸¯äº¤æ˜“æ‰€', 'local'),
                ('0388.HK', 'é¦™æ¸¯äº¤æ˜“æ‰€', 'local'),
                ('0005.hk', 'æ±‡ä¸°æ§è‚¡', 'local'),
                ('0005.HK', 'æ±‡ä¸°æ§è‚¡', 'local'),
            ]
            
            # æ ¹æ®æ•°æ®åº“ç±»å‹ä½¿ç”¨ä¸åŒçš„æ’å…¥è¯­æ³•
            for ticker, company_name, source in local_mappings:
                if IS_PRODUCTION: # V5.0
                    cursor.execute('''
                        INSERT INTO company_names (ticker, company_name, source)
                        VALUES (%s, %s, %s)
                        ON CONFLICT (ticker) DO NOTHING
                    ''', (ticker, company_name, source))
                else:
                    cursor.execute('''
                        INSERT OR IGNORE INTO company_names (ticker, company_name, source)
                        VALUES (?, ?, ?)
                    ''', (ticker, company_name, source))
            print(f"ğŸ“Š åˆå§‹åŒ–äº† {len(local_mappings)} ä¸ªæœ¬åœ°å…¬å¸åç§°æ˜ å°„")
        else:
            print("ğŸ“‹ company_namesè¡¨å·²å­˜åœ¨")
        
        conn.commit()
        cursor.close()
        conn.close()

# Initialize the database when the app starts
init_db()

# å¯åŠ¨æ—¶è‡ªåŠ¨æ›´æ–°è‚¡ç¥¨åå•ç¼“å­˜
print("[INIT] æ£€æŸ¥è‚¡ç¥¨åå•ç¼“å­˜çŠ¶æ€...")
try:
    db = get_db()
    cursor = db.cursor()
    cursor.execute("SELECT COUNT(*) as count FROM company_names WHERE source = 'stock_list_local'")
    local_count = cursor.fetchone()['count']
    cursor.close()
    db.close()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å…¥ï¼ˆå°‘äº1000æ¡è®¤ä¸ºéœ€è¦é‡æ–°å¯¼å…¥ï¼‰
    if local_count < 1000:
        print(f"[INIT] æœ¬åœ°è‚¡ç¥¨åå•ç¼“å­˜ä¸è¶³ï¼ˆ{local_count}æ¡ï¼‰ï¼Œå¼€å§‹è‡ªåŠ¨å¯¼å…¥...")
        update_stock_list_cache()
    else:
        print(f"[INIT] æœ¬åœ°è‚¡ç¥¨åå•ç¼“å­˜å……è¶³ï¼ˆ{local_count}æ¡ï¼‰ï¼Œè·³è¿‡å¯¼å…¥")
        
except Exception as e:
    print(f"[ERROR] æ£€æŸ¥è‚¡ç¥¨åå•ç¼“å­˜å¤±è´¥: {str(e)}")
    print("[INIT] å°è¯•å¼ºåˆ¶å¯¼å…¥è‚¡ç¥¨åå•...")
    try:
        update_stock_list_cache()
    except Exception as e2:
        print(f"[ERROR] å¼ºåˆ¶å¯¼å…¥ä¹Ÿå¤±è´¥: {str(e2)}")

# --- è‚¡ç¥¨ä»£ç æ™ºèƒ½è¯†åˆ«ä¸æ ¼å¼è½¬æ¢ç³»ç»Ÿ ---

def normalize_ticker(user_input):
    """
    å°†ç”¨æˆ·è¾“å…¥æ ‡å‡†åŒ–ä¸ºå†…éƒ¨æ ¼å¼
    æ”¯æŒï¼šçº¯æ•°å­—ä»£ç ã€å¸¦åç¼€ä»£ç ã€å…¬å¸åç§°ã€ç¾è‚¡ä»£ç 
    è¿”å›ï¼š(æ ‡å‡†åŒ–ticker, è¯†åˆ«ç±»å‹)
    """
    if not user_input:
        return None, 'invalid'
    
    user_input = str(user_input).strip()
    print(f"[NORMALIZE] å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if '.' in user_input and user_input.count('.') == 1:
        code, suffix = user_input.split('.')
        suffix = suffix.upper()
        
        # æ ‡å‡†åŒ–åç¼€å
        if suffix in ['SH', 'SZ', 'HK']:
            return f"{code}.{suffix}", 'standard'
        elif suffix == 'SS':  # Yahooæ ¼å¼è½¬å†…éƒ¨æ ¼å¼
            return f"{code}.SH", 'yahoo_format'
        else:
            return user_input.upper(), 'unknown'
    
    # Yahoo Finance ç›´é€šæ ¼å¼ï¼ˆå¦‚ ETH-USD, BTC-USD, EURUSD=X ç­‰ï¼‰
    # è¿™äº›ä»£ç ç›´æ¥ä¼ é€’ç»™ Yahoo Finance APIï¼Œæ— éœ€è½¬æ¢
    if '-' in user_input or user_input.endswith('=X'):
        yahoo_ticker = user_input.upper()
        print(f"[NORMALIZE] æ£€æµ‹åˆ°Yahooç›´é€šæ ¼å¼: {user_input} -> {yahoo_ticker}")
        return yahoo_ticker, 'yahoo_passthrough'
    
    # çº¯è‹±æ–‡å­—æ¯ä»£ç è¯†åˆ«ä¸ºç¾è‚¡ä»£ç 
    if user_input.isalpha() and user_input.isascii():
        us_ticker = user_input.upper()
        print(f"[NORMALIZE] æ£€æµ‹åˆ°ç¾è‚¡ä»£ç : {user_input} -> {us_ticker}")
        return us_ticker, 'us_stock'
    
    # çº¯æ•°å­—ä»£ç æ™ºèƒ½è¯†åˆ«ï¼ˆAè‚¡/æ¸¯è‚¡ï¼‰
    if user_input.isdigit():
        return identify_stock_by_code(user_input)
    
    # å¯èƒ½æ˜¯å…¬å¸åç§°ï¼Œè¿›è¡Œåå‘æŸ¥æ‰¾
    return search_by_company_name(user_input)

def check_ticker_exists(ticker):
    """
    æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦åœ¨æ•°æ®åº“ä¸­å­˜åœ¨
    è¿”å›ï¼š(æ˜¯å¦å­˜åœ¨, å…¬å¸åç§°, æ•°æ®æ¥æº)
    """
    try:
        db = get_db()  # get_db()å·²ç»è®¾ç½®äº†row_factory
        cursor = db.cursor()
        db_execute(cursor, "SELECT company_name, source FROM company_names WHERE ticker = %s", (ticker,))
        result = cursor.fetchone()
        cursor.close()
        db.close()
        
        if result:
            return True, result['company_name'], result['source']
        else:
            return False, None, None
            
    except Exception as e:
        print(f"[ERROR] æ£€æŸ¥è‚¡ç¥¨ä»£ç å­˜åœ¨æ€§å¤±è´¥: {str(e)}")
        return False, None, None

def identify_stock_by_code(code):
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç æ•°å­—è§„å¾‹è¯†åˆ«äº¤æ˜“æ‰€ï¼ˆå¢å¼ºç‰ˆ - æ”¯æŒå†²çªæ£€æµ‹ï¼‰
    """
    original_code = code
    code = code.zfill(6)  # è¡¥é½åˆ°6ä½
    print(f"[IDENTIFY] è¯†åˆ«è‚¡ç¥¨ä»£ç : {original_code} -> {code}")
    
    # å€™é€‰åˆ—è¡¨ï¼šå­˜å‚¨å¯èƒ½çš„æ ¼å¼
    candidates = []
    
    # Aè‚¡ä»£ç è§„å¾‹è¯†åˆ«
    if len(code) == 6:
        first_three = code[:3]
        
        # ä¸Šæµ·äº¤æ˜“æ‰€ï¼ˆæ²ªå¸‚ï¼‰
        if first_three in ['600', '601', '603', '605']:  # æ²ªå¸‚ä¸»æ¿
            candidates.append((f"{code}.SH", 'sh_main'))
        elif first_three == '688':  # ç§‘åˆ›æ¿
            candidates.append((f"{code}.SH", 'sh_star'))
        elif first_three == '689':  # ç§‘åˆ›æ¿
            candidates.append((f"{code}.SH", 'sh_star'))
            
        # æ·±åœ³äº¤æ˜“æ‰€ï¼ˆæ·±å¸‚ï¼‰
        elif first_three in ['000', '001']:  # æ·±å¸‚ä¸»æ¿
            candidates.append((f"{code}.SZ", 'sz_main'))
        elif first_three == '002':  # ä¸­å°æ¿
            candidates.append((f"{code}.SZ", 'sz_sme'))
        elif first_three == '300':  # åˆ›ä¸šæ¿
            candidates.append((f"{code}.SZ", 'sz_gem'))
    
    # æ¸¯è‚¡ä»£ç ï¼ˆä¼˜å…ˆåŸå§‹é•¿åº¦ï¼‰
    if len(original_code) <= 4:
        hk_code = original_code.zfill(4)  # æ¸¯è‚¡è¡¥é½åˆ°4ä½
        candidates.append((f"{hk_code}.HK", 'hk'))
    
    print(f"[IDENTIFY] å€™é€‰æ ¼å¼: {[c[0] for c in candidates]}")
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥å€™é€‰æ ¼å¼æ˜¯å¦å­˜åœ¨
    for ticker, market_type in candidates:
        exists, company_name, source = check_ticker_exists(ticker)
        if exists:
            print(f"[IDENTIFY] æ‰¾åˆ°åŒ¹é…: {original_code} -> {ticker} ({company_name}) [æ¥æº: {source}]")
            return ticker, market_type
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œè¿”å›æœ€å¯èƒ½çš„æ ¼å¼ï¼ˆAè‚¡ä¼˜å…ˆï¼‰
    if candidates:
        fallback_ticker, fallback_type = candidates[0]
        print(f"[IDENTIFY] æ— åŒ¹é…æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼: {original_code} -> {fallback_ticker}")
        return fallback_ticker, fallback_type
    
    # å®Œå…¨æ— æ³•è¯†åˆ«çš„ä»£ç ï¼Œè¿”å›åŸå€¼
    print(f"[WARNING] æ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç : {original_code}")
    return original_code, 'unknown'

def search_by_company_name(company_name):
    """
    æ ¹æ®å…¬å¸åç§°åå‘æŸ¥æ‰¾è‚¡ç¥¨ä»£ç ï¼ˆä¼˜åŒ–ç‰ˆ - æ”¯æŒæ™ºèƒ½ä¼˜å…ˆçº§é€‰æ‹©ï¼‰
    """
    print(f"[SEARCH] æœç´¢å…¬å¸åç§°: {company_name}")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # ç²¾ç¡®åŒ¹é…
        db_execute(cursor, "SELECT ticker FROM company_names WHERE company_name = %s", (company_name,))
        exact_match = cursor.fetchone()
        
        if exact_match:
            ticker = exact_match['ticker']
            print(f"[SEARCH] ç²¾ç¡®åŒ¹é…æ‰¾åˆ°: {company_name} -> {ticker}")
            cursor.close()
            db.close()
            return ticker, 'company_name_exact'
        
        # æ¨¡ç³ŠåŒ¹é…
        db_execute(cursor, "SELECT ticker, company_name FROM company_names WHERE company_name LIKE %s ORDER BY LENGTH(company_name) ASC", (f"%{company_name}%",))
        fuzzy_matches = cursor.fetchall()
        
        if fuzzy_matches:
            print(f"[SEARCH] æ‰¾åˆ° {len(fuzzy_matches)} ä¸ªæ¨¡ç³ŠåŒ¹é…")
            
            if len(fuzzy_matches) == 1:
                ticker = fuzzy_matches[0]['ticker']
                matched_name = fuzzy_matches[0]['company_name']
                print(f"[SEARCH] å•ä¸ªæ¨¡ç³ŠåŒ¹é…: {company_name} -> {ticker} ({matched_name})")
                cursor.close()
                db.close()
                return ticker, 'company_name_fuzzy'
            else:
                # å¤šä¸ªåŒ¹é…æ—¶ï¼Œæ™ºèƒ½é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„
                print(f"[SEARCH] å¤šä¸ªåŒ¹é…ï¼Œåº”ç”¨æ™ºèƒ½ä¼˜å…ˆçº§é€‰æ‹©...")
                
                # ä¼˜å…ˆçº§ï¼šAè‚¡ > æ¸¯è‚¡ > å…¶ä»–ï¼Œä¸”ä¼˜å…ˆstock_list_localæ¥æº
                best_match = None
                best_priority = -1
                
                for match in fuzzy_matches:
                    ticker = match['ticker']
                    matched_name = match['company_name']
                    
                    # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
                    priority = 0
                    
                    # æ•°æ®æ¥æºä¼˜å…ˆçº§
                    db_execute(cursor, "SELECT source FROM company_names WHERE ticker = %s", (ticker,))
                    source_result = cursor.fetchone()
                    source = source_result['source'] if source_result else 'unknown'
                    
                    if source == 'stock_list_local':
                        priority += 1000  # Aè‚¡æœ¬åœ°æ•°æ®æœ€é«˜ä¼˜å…ˆçº§
                    elif source in ['sina_hk', 'alpha_vantage']:
                        priority += 500   # APIæ•°æ®ä¸­ç­‰ä¼˜å…ˆçº§
                    
                    # äº¤æ˜“æ‰€ä¼˜å…ˆçº§
                    if ticker.endswith('.SZ') or ticker.endswith('.SH'):
                        priority += 100   # Aè‚¡ä¼˜å…ˆ
                    elif ticker.endswith('.HK') or ticker.endswith('.hk'):
                        priority += 50    # æ¸¯è‚¡æ¬¡ä¹‹
                    
                    # åç§°åŒ¹é…åº¦ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼Œè¯´æ˜åŒ¹é…åº¦è¶Šé«˜ï¼‰
                    priority += max(0, 50 - len(matched_name))
                    
                    print(f"[SEARCH]   {ticker}: {matched_name} (ä¼˜å…ˆçº§: {priority})")
                    
                    if priority > best_priority:
                        best_priority = priority
                        best_match = (ticker, matched_name)
                
                if best_match:
                    ticker, matched_name = best_match
                    print(f"[SEARCH] æ™ºèƒ½é€‰æ‹©æœ€ä¼˜åŒ¹é…: {company_name} -> {ticker} ({matched_name})")
                    cursor.close()
                    db.close()
                    return ticker, 'company_name_smart_select'
                else:
                    print(f"[SEARCH] æ— æ³•ç¡®å®šæœ€ä¼˜åŒ¹é…")
                    cursor.close()
                    db.close()
                    return None, 'company_name_multiple'
        
        cursor.close()
        db.close()
        print(f"[SEARCH] æœªæ‰¾åˆ°åŒ¹é…çš„å…¬å¸åç§°: {company_name}")
        return None, 'company_name_not_found'
        
    except Exception as e:
        print(f"[ERROR] å…¬å¸åç§°æœç´¢å¤±è´¥: {str(e)}")
        return None, 'search_error'

def generate_smart_error_message(user_input, identification_type):
    """
    æ ¹æ®æœç´¢å¤±è´¥åŸå› ç”Ÿæˆæ™ºèƒ½é”™è¯¯æç¤º
    """
    base_msg = f'æ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç æˆ–å…¬å¸åç§°: {user_input}'
    
    if identification_type == 'company_name_not_found':
        suggestions = [
            "ğŸ’¡ å»ºè®®å°è¯•ï¼š",
            "1. ä½¿ç”¨è‚¡ç¥¨ä»£ç æ›¿ä»£å…¬å¸åç§°ï¼ˆå¦‚ï¼š600000ã€AAPLã€0700ï¼‰",
            "2. æ£€æŸ¥å…¬å¸ç®€ç§°æ˜¯å¦å‡†ç¡®ï¼ˆå¦‚ï¼šä¸­å›½å¹³å®‰ã€å·¥å•†é“¶è¡Œï¼‰",
            "3. å°è¯•ä½¿ç”¨è‹±æ–‡åç§°ï¼ˆç¾è‚¡ï¼‰æˆ–æ•°å­—ä»£ç ï¼ˆAè‚¡/æ¸¯è‚¡ï¼‰"
        ]
        return base_msg + "\n\n" + "\n".join(suggestions)
    
    elif identification_type == 'search_error':
        return base_msg + "\n\nğŸ’¡ å»ºè®®ï¼šç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•æˆ–ä½¿ç”¨è‚¡ç¥¨ä»£ç è¿›è¡Œæœç´¢"
    
    elif identification_type == 'company_name_multiple':
        return base_msg + "\n\nğŸ’¡ å»ºè®®ï¼šå‘ç°å¤šä¸ªåŒ¹é…ç»“æœï¼Œè¯·ä½¿ç”¨æ›´å…·ä½“çš„å…¬å¸åç§°æˆ–ç›´æ¥ä½¿ç”¨è‚¡ç¥¨ä»£ç "
    
    elif user_input.isdigit() and len(user_input) >= 4:
        # å¯èƒ½æ˜¯è‚¡ç¥¨ä»£ç ä½†æ ¼å¼ä¸å¯¹
        return base_msg + f"\n\nğŸ’¡ å»ºè®®ï¼šå¦‚æœè¿™æ˜¯è‚¡ç¥¨ä»£ç ï¼Œè¯·å°è¯•æ ‡å‡†æ ¼å¼ï¼ˆå¦‚ï¼š{user_input}.SHã€{user_input}.SZã€{user_input}.HKï¼‰"
    
    else:
        return base_msg + "\n\nğŸ’¡ æ”¯æŒæ ¼å¼ï¼šAè‚¡ä»£ç ï¼ˆ600000ï¼‰ã€ç¾è‚¡ä»£ç ï¼ˆAAPLï¼‰ã€æ¸¯è‚¡ä»£ç ï¼ˆ0700ï¼‰æˆ–å…¬å¸ç®€ç§°"

def to_yahoo_format(ticker):
    """
    å°†å†…éƒ¨æ ‡å‡†æ ¼å¼è½¬æ¢ä¸ºYahoo Finance APIæ ¼å¼
    å†…éƒ¨æ ¼å¼: 600000.SH, 000001.SZ, 0700.HK
    Yahooæ ¼å¼: 600000.SS, 000001.SZ, 0700.HK
    """
    if not ticker or '.' not in ticker:
        return ticker
    
    code, suffix = ticker.split('.')
    
    # ä¸Šæµ·äº¤æ˜“æ‰€ï¼š.SH -> .SS
    if suffix == 'SH':
        yahoo_ticker = f"{code}.SS"
        print(f"[FORMAT] è½¬æ¢Yahooæ ¼å¼: {ticker} -> {yahoo_ticker}")
        return yahoo_ticker
    
    # æ·±åœ³äº¤æ˜“æ‰€å’Œæ¸¯è‚¡ä¿æŒä¸å˜
    elif suffix in ['SZ', 'HK']:
        return ticker
    
    # æœªçŸ¥åç¼€ä¿æŒåŸæ ·
    else:
        return ticker

def to_display_format(ticker):
    """
    å°†tickerè½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„æ˜¾ç¤ºæ ¼å¼
    """
    if not ticker or '.' not in ticker:
        return ticker
    
    code, suffix = ticker.split('.')
    
    if suffix == 'SH':
        return f"{code}(æ²ªå¸‚)"
    elif suffix == 'SZ':
        return f"{code}(æ·±å¸‚)"
    elif suffix == 'HK':
        return f"{code}(æ¸¯è‚¡)"
    else:
        return ticker

# --- å…¬å¸åç§°ç¼“å­˜ç®¡ç†å‡½æ•° ---
def get_cached_company_name(ticker):
    """ä»æ•°æ®åº“ç¼“å­˜ä¸­è·å–å…¬å¸åç§°"""
    try:
        db = get_db()
        cursor = db.cursor()
        db_execute(cursor, "SELECT company_name, source FROM company_names WHERE ticker = %s", (ticker,))
        result = cursor.fetchone()
        cursor.close()
        db.close()
        
        if result:
            print(f"[CACHE] ä»æ•°æ®åº“è·å–å…¬å¸åç§°: {ticker} -> {result['company_name']} (æ¥æº: {result['source']})")
            return result['company_name']
        return None
    except Exception as e:
        print(f"[ERROR] æŸ¥è¯¢ç¼“å­˜å¤±è´¥: {e}")
        return None

def save_company_name_to_cache(ticker, company_name, source='api'):
    """å°†å…¬å¸åç§°ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜"""
    try:
        db = get_db()
        cursor = db.cursor()
        db_execute(cursor, '''
            INSERT OR REPLACE INTO company_names (ticker, company_name, source, last_updated)
            VALUES (%s, %s, %s, CURRENT_TIMESTAMP)
        ''', (ticker, company_name, source))
        db.commit()
        cursor.close()
        db.close()
        print(f"[CACHE] ä¿å­˜å…¬å¸åç§°åˆ°ç¼“å­˜: {ticker} -> {company_name} (æ¥æº: {source})")
        return True
    except Exception as e:
        print(f"[ERROR] ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        return False

# æ·»åŠ æµè§ˆå™¨ User-Agentï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_company_name(ticker):
    """è·å–è‚¡ç¥¨ä»£ç å¯¹åº”çš„å…¬å¸åç§° - å¤šå±‚çº§æŸ¥è¯¢æœºåˆ¶ï¼ˆå®¹é”™å¢å¼ºç‰ˆï¼‰"""
    print(f"[DEBUG] å¼€å§‹è·å–å…¬å¸åç§°: {ticker}")
    
    if not ticker:
        print("[WARNING] tickerä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼")
        return "æœªçŸ¥è‚¡ç¥¨"
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºAè‚¡ä»£ç 
    is_a_stock = (ticker.endswith('.SH') or ticker.endswith('.SZ'))
    
    if is_a_stock:
        print(f"[PRIORITY] æ£€æµ‹åˆ°Aè‚¡ä»£ç ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ•°æ®: {ticker}")
        
        # Aè‚¡å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ•°æ®ï¼Œä¸è°ƒç”¨API
        cached_name = get_cached_company_name(ticker)
        if cached_name:
            print(f"[SUCCESS] Aè‚¡æœ¬åœ°æ•°æ®: {ticker} -> {cached_name}")
            return cached_name
        
        # å¦‚æœæœ¬åœ°æ•°æ®ä¸å­˜åœ¨ï¼Œå°è¯•ä¸åŒå¤§å°å†™æ ¼å¼
        ticker_variants = [ticker.upper(), ticker.lower()]
        for variant in ticker_variants:
            if variant != ticker:
                cached_name = get_cached_company_name(variant)
                if cached_name:
                    # å°†ç»“æœä¹Ÿç¼“å­˜åˆ°åŸå§‹tickerä¸‹
                    save_company_name_to_cache(ticker, cached_name, 'cache_alias')
                    print(f"[SUCCESS] Aè‚¡å˜ä½“åŒ¹é…: {ticker} -> {cached_name}")
                    return cached_name
        
        # Aè‚¡æ‰¾ä¸åˆ°æ•°æ®æ—¶ï¼Œè¿”å›ç¾åŒ–çš„ä»£ç æ˜¾ç¤ºï¼Œä¸è°ƒç”¨API
        display_name = ticker
        if ticker.endswith('.SH'):
            display_name = f"{ticker.replace('.SH', '')}(æ²ªå¸‚)"
        elif ticker.endswith('.SZ'):
            display_name = f"{ticker.replace('.SZ', '')}(æ·±å¸‚)"
        
        print(f"[FALLBACK] Aè‚¡æœ¬åœ°æ•°æ®ç¼ºå¤±ï¼Œä½¿ç”¨ç¾åŒ–æ˜¾ç¤º: {ticker} -> {display_name}")
        save_company_name_to_cache(ticker, display_name, 'a_stock_fallback')
        return display_name
    
    else:
        print(f"[PRIORITY] éAè‚¡ä»£ç ï¼Œä½¿ç”¨å®Œæ•´æŸ¥è¯¢é“¾: {ticker}")
        
        # éAè‚¡ï¼šæ­£å¸¸çš„å¤šå±‚çº§æŸ¥è¯¢ï¼ˆæœ¬åœ°ç¼“å­˜ â†’ APIè°ƒç”¨ï¼‰
        # ç¬¬ä¸€å±‚ï¼šæ£€æŸ¥æ•°æ®åº“ç¼“å­˜
        cached_name = get_cached_company_name(ticker)
        if cached_name:
            return cached_name
        
        # ç¬¬äºŒå±‚ï¼šå°è¯•ä¸åŒå¤§å°å†™æ ¼å¼çš„ticker
        ticker_variants = [ticker, ticker.upper(), ticker.lower()]
        for variant in ticker_variants:
            if variant != ticker:  # é¿å…é‡å¤æŸ¥è¯¢
                cached_name = get_cached_company_name(variant)
                if cached_name:
                    # å°†ç»“æœä¹Ÿç¼“å­˜åˆ°åŸå§‹tickerä¸‹
                    save_company_name_to_cache(ticker, cached_name, 'cache_alias')
                    return cached_name
        
        # ç¬¬ä¸‰å±‚ï¼šAPIè°ƒç”¨ï¼ˆä»…ç”¨äºæ¸¯è‚¡ã€ç¾è‚¡ç­‰ï¼‰
        api_result = fetch_company_name_from_api(ticker)
        if api_result:
            return api_result
        
        # ç¬¬å››å±‚ï¼šæœ€ç»ˆå®¹é”™æœºåˆ¶
        print(f"[WARNING] æ— æ³•è·å–å…¬å¸åç§°ï¼Œä½¿ç”¨è‚¡ç¥¨ä»£ç ä½œä¸ºæ˜¾ç¤ºåç§°: {ticker}")
        
        # å°è¯•ç¾åŒ–è‚¡ç¥¨ä»£ç æ˜¾ç¤º
        display_name = ticker
        
        # ä¸ºæ¸¯è‚¡ã€ç¾è‚¡ä»£ç æ·»åŠ æ ‡è¯†
        if ticker.endswith('.hk') or ticker.endswith('.HK'):
            display_name = f"{ticker}(é¦™æ¸¯)"
        elif '-' in ticker:  # åŠ å¯†è´§å¸/å¤–æ±‡å¯¹ï¼ˆå¦‚ ETH-USD, BTC-USDï¼‰
            display_name = f"{ticker}"
        elif '.' not in ticker and ticker.isalpha():  # ç¾è‚¡ä»£ç 
            display_name = f"{ticker}(ç¾è‚¡)"
        
        # ä¿å­˜åˆ°ç¼“å­˜ï¼Œé¿å…é‡å¤æŸ¥è¯¢
        save_company_name_to_cache(ticker, display_name, 'fallback')
        
        return display_name

def fetch_company_name_from_sina_hk(ticker):
    """ä»æ–°æµªè´¢ç»APIè·å–æ¸¯è‚¡å…¬å¸åç§°"""
    print(f"[API] å°è¯•ä»æ–°æµªè´¢ç»APIè·å–æ¸¯è‚¡å…¬å¸åç§°: {ticker}")
    
    try:
        # æ¸¯è‚¡ä»£ç æ ¼å¼ï¼šhk + å»æ‰.hkçš„ä»£ç 
        code = ticker.replace('.hk', '').replace('.HK', '')
        sina_code = f"hk{code.zfill(5)}"  # è¡¥é½åˆ°5ä½
        
        url = f"https://hq.sinajs.cn/list={sina_code}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://finance.sina.com.cn/'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        print(f"[API] æ–°æµªè´¢ç»æ¸¯è‚¡ å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200 and response.text:
            # è§£ææ–°æµªè´¢ç»è¿”å›æ ¼å¼
            content = response.text
            if 'var hq_str_' in content:
                data_part = content.split('="')[1].split('";')[0]
                fields = data_part.split(',')
                if len(fields) > 1:
                    company_name = fields[1]  # ç¬¬äºŒä¸ªå­—æ®µé€šå¸¸æ˜¯å…¬å¸åç§°
                    print(f"[SUCCESS] æ–°æµªè´¢ç»è·å–åˆ°æ¸¯è‚¡å…¬å¸åç§°: {ticker} -> {company_name}")
                    save_company_name_to_cache(ticker, company_name, 'sina_hk')
                    return company_name
        
        print(f"[API] æ–°æµªè´¢ç»æ¸¯è‚¡æœªæ‰¾åˆ°åŒ¹é…: {ticker}")
        
    except Exception as e:
        print(f"[ERROR] æ–°æµªè´¢ç»æ¸¯è‚¡APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    return None

def fetch_company_name_from_api(ticker):
    """ä»å¤šä¸ªAPIè·å–å…¬å¸åç§° - å¸¦æ¸¯è‚¡æ”¯æŒ"""
    print(f"[API] å°è¯•ä»APIè·å–å…¬å¸åç§°: {ticker}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¸¯è‚¡ä»£ç 
    is_hk_stock = ticker.lower().endswith('.hk')
    
    if is_hk_stock:
        # å¯¹äºæ¸¯è‚¡ï¼Œä¼˜å…ˆä½¿ç”¨æ–°æµªè´¢ç»API
        print(f"[API] æ£€æµ‹åˆ°æ¸¯è‚¡ä»£ç ï¼Œä½¿ç”¨æ–°æµªè´¢ç»API: {ticker}")
        hk_result = fetch_company_name_from_sina_hk(ticker)
        if hk_result:
            return hk_result
        print(f"[API] æ–°æµªè´¢ç»å¤±è´¥ï¼Œå°è¯•Alpha Vantageä½œä¸ºå¤‡é€‰...")
    
    # Alpha Vantage API (ç¾è‚¡ä¸»åŠ› + æ¸¯è‚¡å¤‡é€‰)
    alpha_vantage_key = "BT4ER0H28HOFCY3R"
    
    try:
        url = "https://www.alphavantage.co/query"
        params = {
            'function': 'SYMBOL_SEARCH',
            'keywords': ticker,
            'apikey': alpha_vantage_key
        }
        
        response = requests.get(url, params=params, headers=HEADERS, timeout=15)
        print(f"[API] Alpha Vantage å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            
            # æ£€æŸ¥APIé™åˆ¶
            if 'Note' in data:
                print(f"[API] Alpha Vantage APIé™åˆ¶: {data['Note']}")
                return None
            
            if 'bestMatches' in data and len(data['bestMatches']) > 0:
                best_match = data['bestMatches'][0]
                company_name = best_match.get('2. name')
                match_symbol = best_match.get('1. symbol')
                region = best_match.get('4. region', '')
                match_score = best_match.get('9. matchScore', '0')
                
                print(f"[API] æ‰¾åˆ°åŒ¹é…: {match_symbol} -> {company_name}")
                print(f"[API] åœ°åŒº: {region}, åŒ¹é…åº¦: {match_score}")
                
                if company_name and float(match_score) > 0.5:  # åªæ¥å—åŒ¹é…åº¦>0.5çš„ç»“æœ
                    print(f"[SUCCESS] Alpha Vantageè·å–åˆ°å…¬å¸åç§°: {ticker} -> {company_name}")
                    save_company_name_to_cache(ticker, company_name, 'alpha_vantage')
                    return company_name
                else:
                    print(f"[API] åŒ¹é…åº¦è¿‡ä½æˆ–æ— å…¬å¸åç§°ï¼Œè·³è¿‡")
            else:
                print(f"[API] Alpha Vantageæœªæ‰¾åˆ°åŒ¹é…: {ticker}")
        else:
            print(f"[API] Alpha Vantageè¯·æ±‚å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        print(f"[ERROR] Alpha Vantage APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    print(f"[WARNING] æ‰€æœ‰APIéƒ½æ— æ³•è·å–å…¬å¸åç§°: {ticker}")
    return None

# --- Aè‚¡è‚¡ç¥¨åå•ç¼“å­˜ç³»ç»Ÿ ---
def fetch_sz_stock_list():
    """ä»æ·±åœ³äº¤æ˜“æ‰€APIè·å–è‚¡ç¥¨åå•"""
    print("[STOCK_LIST] å¼€å§‹è·å–æ·±åœ³äº¤æ˜“æ‰€è‚¡ç¥¨åå•...")
    
    try:
        url = "http://api.biyingapi.com/hslt/list/biyinglicence"
        response = requests.get(url, headers=HEADERS, timeout=30)
        print(f"[STOCK_LIST] æ·±åœ³äº¤æ˜“æ‰€APIå“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            if isinstance(data, list):
                print(f"[STOCK_LIST] æˆåŠŸè·å–æ·±åœ³äº¤æ˜“æ‰€è‚¡ç¥¨æ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
            else:
                print(f"[ERROR] æ·±åœ³äº¤æ˜“æ‰€APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}")
                return []
        else:
            print(f"[ERROR] æ·±åœ³äº¤æ˜“æ‰€APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return []
            
    except Exception as e:
        print(f"[ERROR] è·å–æ·±åœ³äº¤æ˜“æ‰€è‚¡ç¥¨åå•å¤±è´¥: {str(e)}")
        return []

def fetch_sh_stock_list():
    """ä»ä¸Šæµ·äº¤æ˜“æ‰€ç›¸å…³APIè·å–è‚¡ç¥¨åå•ï¼ˆå¾…å®ç°ï¼‰"""
    print("[STOCK_LIST] ä¸Šæµ·äº¤æ˜“æ‰€è‚¡ç¥¨åå•è·å–åŠŸèƒ½å¾…å®ç°...")
    
    # TODO: æŸ¥æ‰¾ä¸Šæµ·äº¤æ˜“æ‰€çš„è‚¡ç¥¨åå•API
    # å¯èƒ½çš„APIæ¥æºï¼š
    # 1. åŒèŠ±é¡ºAPI
    # 2. ä¸œæ–¹è´¢å¯ŒAPI  
    # 3. æ–°æµªè´¢ç»API
    # 4. å…¶ä»–é‡‘èæ•°æ®æº
    
    return []

def save_stock_list_to_cache(stock_data, exchange, data_version=None):
    """å°†è‚¡ç¥¨åå•æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜"""
    if not stock_data:
        print("[STOCK_LIST] æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return 0
        
    print(f"[STOCK_LIST] å¼€å§‹ä¿å­˜ {exchange} äº¤æ˜“æ‰€è‚¡ç¥¨åå•ï¼Œå…± {len(stock_data)} æ¡è®°å½•...")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        saved_count = 0
        updated_count = 0
        
        for stock in stock_data:
            # æ·±åœ³äº¤æ˜“æ‰€æ•°æ®æ ¼å¼ï¼š{"dm": "000001.SZ", "mc": "å¹³å®‰é“¶è¡Œ", "jys": "SZ"}
            if exchange == 'SZ':
                ticker = stock.get('dm', '').strip()
                company_name = stock.get('mc', '').strip()
            else:
                # å…¶ä»–äº¤æ˜“æ‰€çš„æ•°æ®æ ¼å¼å¾…å®š
                continue
                
            if not ticker or not company_name:
                continue
                
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            db_execute(cursor, "SELECT ticker FROM company_names WHERE ticker = %s", (ticker,))
            existing = cursor.fetchone()
            
            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                cursor.execute('''
                    UPDATE company_names 
                    SET company_name = %s, source = %s, last_updated = CURRENT_TIMESTAMP
                    WHERE ticker = %s
                ''', (company_name, f'stock_list_{exchange.lower()}', ticker))
                updated_count += 1
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute('''
                    INSERT INTO company_names (ticker, company_name, source, last_updated)
                    VALUES (%s, %s, %s, CURRENT_TIMESTAMP)
                ''', (ticker, company_name, f'stock_list_{exchange.lower()}'))
                saved_count += 1
        
        db.commit()
        cursor.close()
        db.close()
        
        print(f"[STOCK_LIST] ä¿å­˜å®Œæˆ - æ–°å¢: {saved_count}, æ›´æ–°: {updated_count}")
        return saved_count + updated_count
        
    except Exception as e:
        print(f"[ERROR] ä¿å­˜è‚¡ç¥¨åå•åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
        return 0

def load_local_stock_list():
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½Aè‚¡è‚¡ç¥¨åå•"""
    print("[STOCK_LIST] å¼€å§‹ä»æœ¬åœ°æ–‡ä»¶åŠ è½½Aè‚¡è‚¡ç¥¨åå•...")
    
    try:
        # æœ¬åœ°æ–‡ä»¶è·¯å¾„
        local_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'Aè‚¡å…¬å¸è¯åˆ¸ä»£ç å’Œå…¬å¸åç§°.md')
        
        if not os.path.exists(local_file):
            print(f"[ERROR] æœ¬åœ°è‚¡ç¥¨åå•æ–‡ä»¶ä¸å­˜åœ¨: {local_file}")
            return []
        
        with open(local_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            print(f"[STOCK_LIST] æˆåŠŸè¯»å–æœ¬åœ°è‚¡ç¥¨åå•ï¼Œå…± {len(data)} æ¡è®°å½•")
            return data
        else:
            print(f"[ERROR] æœ¬åœ°æ–‡ä»¶æ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}")
            return []
            
    except Exception as e:
        print(f"[ERROR] è¯»å–æœ¬åœ°è‚¡ç¥¨åå•å¤±è´¥: {str(e)}")
        return []

def save_local_stock_list_to_cache(stock_data):
    """å°†æœ¬åœ°è‚¡ç¥¨åå•æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ç¼“å­˜"""
    if not stock_data:
        print("[STOCK_LIST] æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return 0
        
    print(f"[STOCK_LIST] å¼€å§‹æ‰¹é‡ä¿å­˜Aè‚¡è‚¡ç¥¨åå•ï¼Œå…± {len(stock_data)} æ¡è®°å½•...")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        saved_count = 0
        updated_count = 0
        
        for stock in stock_data:
            # æ•°æ®æ ¼å¼ï¼š{"dm": "000001.SZ", "mc": "å¹³å®‰é“¶è¡Œ", "jys": "SZ"}
            ticker = stock.get('dm', '').strip()
            company_name = stock.get('mc', '').strip()
            exchange = stock.get('jys', '').strip()
                
            if not ticker or not company_name:
                continue
                
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            db_execute(cursor, "SELECT ticker, source FROM company_names WHERE ticker = %s", (ticker,))
            existing = cursor.fetchone()
            
            if existing:
                # åªæœ‰å½“ç°æœ‰æ•°æ®ä¸æ˜¯æ¥è‡ªæœ¬åœ°è‚¡ç¥¨åå•æ—¶æ‰æ›´æ–°
                if existing['source'] != 'stock_list_local':
                    cursor.execute('''
                        UPDATE company_names 
                        SET company_name = %s, source = %s, last_updated = CURRENT_TIMESTAMP
                        WHERE ticker = %s
                    ''', (company_name, 'stock_list_local', ticker))
                    updated_count += 1
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute('''
                    INSERT INTO company_names (ticker, company_name, source, last_updated)
                    VALUES (%s, %s, %s, CURRENT_TIMESTAMP)
                ''', (ticker, company_name, 'stock_list_local'))
                saved_count += 1
        
        db.commit()
        cursor.close()
        db.close()
        
        print(f"[STOCK_LIST] æ‰¹é‡ä¿å­˜å®Œæˆ - æ–°å¢: {saved_count}, æ›´æ–°: {updated_count}")
        return saved_count + updated_count
        
    except Exception as e:
        print(f"[ERROR] æ‰¹é‡ä¿å­˜è‚¡ç¥¨åå•åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
        return 0

def update_stock_list_cache():
    """æ›´æ–°è‚¡ç¥¨åå•ç¼“å­˜ - ä¸»å…¥å£å‡½æ•°"""
    print("[STOCK_LIST] ========== å¼€å§‹æ›´æ–°è‚¡ç¥¨åå•ç¼“å­˜ ==========")
    
    total_saved = 0
    
    # 1. ä¼˜å…ˆåŠ è½½æœ¬åœ°å®Œæ•´Aè‚¡æ•°æ®
    local_data = load_local_stock_list()
    if local_data:
        local_count = save_local_stock_list_to_cache(local_data)
        total_saved += local_count
        print(f"[STOCK_LIST] æœ¬åœ°Aè‚¡æ•°æ®å¤„ç†å®Œæˆ: {local_count} æ¡")
    
    # 2. å¤‡ç”¨ï¼šè·å–æ·±åœ³äº¤æ˜“æ‰€æ•°æ®ï¼ˆAPIæ–¹å¼ï¼‰
    if not local_data:
        print("[STOCK_LIST] æœ¬åœ°æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•APIæ–¹å¼...")
        sz_data = fetch_sz_stock_list()
        if sz_data:
            sz_count = save_stock_list_to_cache(sz_data, 'SZ')
            total_saved += sz_count
            print(f"[STOCK_LIST] æ·±åœ³äº¤æ˜“æ‰€APIæ•°æ®å¤„ç†å®Œæˆ: {sz_count} æ¡")
    
    print(f"[STOCK_LIST] ========== è‚¡ç¥¨åå•ç¼“å­˜æ›´æ–°å®Œæˆï¼Œæ€»è®¡: {total_saved} æ¡ ==========")
    return total_saved

def calculate_zig(series, threshold):
    if series.isnull().all():
        return [None] * len(series)

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼ä½œä¸ºèµ·ç‚¹
    first_valid_index = series.first_valid_index()
    if first_valid_index is None:
        return [None] * len(series)

    threshold = threshold / 100.0
    trend = 0  # 0: TBD, 1: up, -1: down
    last_pivot_price = series[first_valid_index]
    last_pivot_index = first_valid_index
    pivots = {last_pivot_index: last_pivot_price}

    for i in range(first_valid_index + 1, len(series)):
        current_price = series.iloc[i]
        if pd.isna(current_price):
            continue

        if trend == 0:
            if current_price / last_pivot_price > 1 + threshold:
                trend = 1
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i
            elif current_price / last_pivot_price < 1 - threshold:
                trend = -1
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i
        elif trend == 1:
            if current_price > last_pivot_price:
                pivots.pop(last_pivot_index)
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i
            elif current_price / last_pivot_price < 1 - threshold:
                trend = -1
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i
        elif trend == -1:
            if current_price < last_pivot_price:
                pivots.pop(last_pivot_index)
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i
            elif current_price / last_pivot_price > 1 + threshold:
                trend = 1
                pivots[i] = current_price
                last_pivot_price = current_price
                last_pivot_index = i

    zig_series = pd.Series([np.nan] * len(series), index=series.index)
    for index, value in pivots.items():
        zig_series.loc[index] = value

    # å…³é”®ä¿®å¤ï¼šå°†æ‰€æœ‰NaNæ›¿æ¢ä¸ºNoneï¼Œä»¥ä¾¿æ­£ç¡®è½¬æ¢ä¸ºJSONçš„null
    return [None if pd.isna(x) else x for x in zig_series]

def calculate_phases_from_zig(zig_series, timestamps):
    import datetime as dt
    pivots = [(i, v) for i, v in enumerate(zig_series) if v is not None]
    if len(pivots) < 2:
        return []

    # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰pivotç´¢å¼•éƒ½åœ¨timestampsèŒƒå›´å†…
    max_index = len(timestamps) - 1
    valid_pivots = [(i, v) for i, v in pivots if i <= max_index]

    if len(valid_pivots) < 2:
        print(f"[WARNING] æœ‰æ•ˆpivotæ•°é‡ä¸è¶³: {len(valid_pivots)}, timestampsé•¿åº¦: {len(timestamps)}")
        return []

    phases = []
    for i in range(len(valid_pivots) - 1):
        start_index, start_value = valid_pivots[i]
        end_index, end_value = valid_pivots[i+1]

        # åŒé‡æ£€æŸ¥è¾¹ç•Œ
        if start_index > max_index or end_index > max_index:
            print(f"[ERROR] ç´¢å¼•è¶Šç•Œ: start={start_index}, end={end_index}, max={max_index}")
            continue

        start_date = dt.datetime.fromtimestamp(timestamps[start_index]).strftime('%Y-%m-%d')
        end_date = dt.datetime.fromtimestamp(timestamps[end_index]).strftime('%Y-%m-%d')

        phase_type = 'Uptrend' if end_value > start_value else 'Downtrend'

        phases.append({
            'start_date': start_date,
            'end_date': end_date,
            'phase': phase_type
        })
    return phases


@app.route('/')
@login_required
def index():
    return render_template('index.html')

@app.route('/test')
@login_required
def test():
    from flask import send_file
    return send_file('test_zig.html')

@app.route('/test-markarea')
@login_required
def test_markarea():
    from flask import send_file
    return send_file('test_markarea.html')

# V5.0: æ–°å¢ç™»å½•/ç™»å‡ºè·¯ç”±
@app.route('/login', methods=['GET', 'POST'])
def login():
    error = None
    if request.method == 'POST':
        if request.form['password'] == APP_PASSWORD:
            session['logged_in'] = True
            session.permanent = True # ç¡®ä¿ä¼šè¯æŒä¹…åŒ–
            next_url = request.args.get('next')
            print(f"[AUTH] ç™»å½•æˆåŠŸ. é‡å®šå‘åˆ°: {next_url or url_for('index')}")
            return redirect(next_url or url_for('index'))
        else:
            error = 'å¯†ç é”™è¯¯ï¼Œè¯·é‡è¯•'
            print("[AUTH] ç™»å½•å¤±è´¥: å¯†ç æ— æ•ˆ")
    return render_template('login.html', error=error)

@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    print("[AUTH] ç”¨æˆ·å·²ç™»å‡º.")
    return redirect(url_for('login'))


# --- V3.1: æ–°å¢APIç”¨äºå¤„ç†æ‰‹åŠ¨æ³¨é‡Š ---
@app.route('/api/annotation', methods=['POST'])
@require_api_auth
def add_annotation():
    data = request.get_json()
    if not data or not all(k in data for k in ['ticker', 'date', 'text', 'id']):
        return jsonify({'error': 'Missing data'}), 400
    
    try:
        # æ ‡å‡†åŒ–tickerä»¥ç¡®ä¿ä¸€è‡´æ€§
        normalized_ticker, _ = normalize_ticker(data['ticker'])
        if not normalized_ticker:
            return jsonify({'error': 'Invalid ticker format'}), 400
        
        db = get_db()
        cursor = db.cursor()
        
        # è·å–annotation_typeï¼Œå¦‚æœå‰ç«¯æ²¡æœ‰æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼'manual'
        annotation_type = data.get('type', 'manual')
        
        # è·å–AIåˆ†æç›¸å…³çš„é¢å¤–å­—æ®µ
        algorithm_type = data.get('algorithm_type')
        source_annotation_id = data.get('source_annotation_id')
        
        # å‡†å¤‡æ’å…¥çš„æ•°æ®
        insert_data = [normalized_ticker, data['date'], data['text'], data['id'], annotation_type]
        
        # æ„å»ºSQLè¯­å¥ï¼Œæ”¯æŒAIåˆ†æå­—æ®µ
        if algorithm_type:
            sql = """
                INSERT INTO annotations 
                (ticker, date, text, annotation_id, annotation_type, algorithm_type, created_at, updated_at) 
                VALUES (%s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """
            insert_data.append(algorithm_type)
        else:
            sql = """
                INSERT INTO annotations 
                (ticker, date, text, annotation_id, annotation_type, created_at, updated_at) 
                VALUES (%s, %s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """
        
        db_execute(cursor, sql, insert_data)
        db.commit()
        
        # è®°å½•AIåˆ†ææ—¥å¿—
        if algorithm_type == 'ai_analysis':
            print(f"[AIåˆ†æ] ä¿å­˜æˆåŠŸ: {data['id']} for {normalized_ticker} on {data['date']}")
            if source_annotation_id:
                print(f"[AIåˆ†æ] æºæ³¨é‡ŠID: {source_annotation_id}")
        
        return jsonify({'success': True, 'message': 'Annotation added'}), 201
        
    except sqlite3.IntegrityError:
        # å¦‚æœ annotation_id å·²å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯é‡è¯•ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯æˆåŠŸçš„
        return jsonify({'success': True, 'message': 'Annotation already exists'}), 200
    except Exception as e:
        print(f"[ERROR] ä¿å­˜æ³¨é‡Šå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()

@app.route('/api/annotation/<string:annotation_id>', methods=['DELETE'])
@require_api_auth
def delete_annotation(annotation_id):
    # URLè§£ç å¤„ç†
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] åˆ é™¤æ³¨é‡ŠAPIè°ƒç”¨")
    print(f"[DEBUG] åŸå§‹annotation_id: '{annotation_id}'")
    print(f"[DEBUG] è§£ç åannotation_id: '{decoded_id}'")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨ä¸”æœªåˆ é™¤ - åŒæ—¶ç”¨åŸå§‹IDå’Œè§£ç IDè¿›è¡ŒæŸ¥è¯¢
        db_execute(cursor, """
            SELECT annotation_id, annotation_type FROM annotations 
            WHERE (annotation_id = %s OR annotation_id = %s) AND is_deleted = 0
        """, (annotation_id, decoded_id))
        existing = cursor.fetchone()
        print(f"[DEBUG] æŸ¥è¯¢ç°æœ‰è®°å½•: {existing}")
        
        if not existing:
            print(f"[ERROR] æ³¨é‡Šæœªæ‰¾åˆ°æˆ–å·²åˆ é™¤")
            return jsonify({'error': 'Annotation not found'}), 404
        
        # ä½¿ç”¨æŸ¥è¯¢åˆ°çš„å®é™…IDè¿›è¡Œè½¯åˆ é™¤
        actual_id = existing['annotation_id']
        print(f"[DEBUG] ä½¿ç”¨å®é™…IDè¿›è¡Œè½¯åˆ é™¤: '{actual_id}'")
        
        # è½¯åˆ é™¤ï¼šè®¾ç½® is_deleted = 1 å’Œ deleted_at æ—¶é—´æˆ³
        db_execute(cursor, """
            UPDATE annotations 
            SET is_deleted = 1, deleted_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
            WHERE annotation_id = %s
        """, (actual_id,))
        db.commit()
        
        print(f"[DEBUG] è½¯åˆ é™¤æˆåŠŸ: {cursor.rowcount} è¡Œå—å½±å“")
        
        if cursor.rowcount == 0:
            return jsonify({'error': 'Annotation not found'}), 404
        
        return jsonify({'success': True, 'message': 'Annotation moved to recycle bin'}), 200
    except Exception as e:
        print(f"[ERROR] æ•°æ®åº“æ“ä½œå¼‚å¸¸: {str(e)}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()

@app.route('/api/annotations/favorite/<string:annotation_id>', methods=['POST'])
@require_api_auth
def mark_annotation_favorite(annotation_id):
    """æ ‡è®°æ³¨é‡Šä¸ºé‡ç‚¹"""
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] æ ‡è®°é‡ç‚¹æ³¨é‡ŠAPIè°ƒç”¨: '{decoded_id}'")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æŸ¥æ‰¾å¹¶æ›´æ–°æ³¨é‡Š
        db_execute(cursor, """
            UPDATE annotations 
            SET is_favorite = 1, updated_at = CURRENT_TIMESTAMP
            WHERE (annotation_id = %s OR annotation_id = %s) AND is_deleted = 0
        """, (annotation_id, decoded_id))
        
        if cursor.rowcount == 0:
            return jsonify({'error': 'Annotation not found or already deleted'}), 404
        
        db.commit()
        print(f"[DEBUG] æ³¨é‡Šæ ‡è®°ä¸ºé‡ç‚¹æˆåŠŸ: {cursor.rowcount} è¡Œå—å½±å“")
        
        return jsonify({'success': True, 'message': 'Annotation marked as favorite'}), 200
    except Exception as e:
        print(f"[ERROR] æ ‡è®°é‡ç‚¹æ³¨é‡Šå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()

@app.route('/api/annotations/favorite/<string:annotation_id>', methods=['DELETE'])
@require_api_auth
def unmark_annotation_favorite(annotation_id):
    """å–æ¶ˆæ³¨é‡Šé‡ç‚¹æ ‡è®°"""
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] å–æ¶ˆé‡ç‚¹æ ‡è®°APIè°ƒç”¨: '{decoded_id}'")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æŸ¥æ‰¾å¹¶æ›´æ–°æ³¨é‡Š
        db_execute(cursor, """
            UPDATE annotations 
            SET is_favorite = 0, updated_at = CURRENT_TIMESTAMP
            WHERE (annotation_id = %s OR annotation_id = %s) AND is_deleted = 0
        """, (annotation_id, decoded_id))
        
        if cursor.rowcount == 0:
            return jsonify({'error': 'Annotation not found or already deleted'}), 404
        
        db.commit()
        print(f"[DEBUG] æ³¨é‡Šå–æ¶ˆé‡ç‚¹æ ‡è®°æˆåŠŸ: {cursor.rowcount} è¡Œå—å½±å“")
        
        return jsonify({'success': True, 'message': 'Annotation unmarked as favorite'}), 200
    except Exception as e:
        print(f"[ERROR] å–æ¶ˆé‡ç‚¹æ ‡è®°å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()

@app.route('/api/annotation/<string:annotation_id>', methods=['PUT'])
@require_api_auth
def update_annotation(annotation_id):
    # URLè§£ç å¤„ç†
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] ç¼–è¾‘æ³¨é‡ŠAPIè°ƒç”¨")
    print(f"[DEBUG] åŸå§‹annotation_id: '{annotation_id}'")
    print(f"[DEBUG] è§£ç åannotation_id: '{decoded_id}'")
    
    data = request.get_json()
    print(f"[DEBUG] æ¥æ”¶åˆ°çš„æ•°æ®: {data}")
    
    if not data or not all(k in data for k in ['date', 'text']):
        print(f"[ERROR] ç¼ºå°‘å¿…è¦çš„æ•°æ®å­—æ®µ")
        return jsonify({'error': 'Missing date or text'}), 400
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨ - åŒæ—¶ç”¨åŸå§‹IDå’Œè§£ç IDè¿›è¡ŒæŸ¥è¯¢
        db_execute(cursor, "SELECT * FROM annotations WHERE annotation_id = %s OR annotation_id = %s", 
                      (annotation_id, decoded_id))
        existing = cursor.fetchone()
        print(f"[DEBUG] æŸ¥è¯¢ç°æœ‰è®°å½•: {existing}")
        
        if not existing:
            print(f"[ERROR] æ³¨é‡Šæœªæ‰¾åˆ°")
            print(f"[ERROR] å°è¯•çš„ID: '{annotation_id}' å’Œ '{decoded_id}'")
            return jsonify({'error': 'Annotation not found'}), 404
        
        # ä½¿ç”¨æŸ¥è¯¢åˆ°çš„å®é™…IDè¿›è¡Œæ›´æ–°
        actual_id = existing['annotation_id']
        print(f"[DEBUG] ä½¿ç”¨å®é™…IDè¿›è¡Œæ›´æ–°: '{actual_id}'")
        
        # æ›´æ–°è®°å½•ï¼ŒåŒæ—¶æ›´æ–°æ—¶é—´æˆ³
        db_execute(cursor,
            "UPDATE annotations SET date = %s, text = %s, updated_at = CURRENT_TIMESTAMP WHERE annotation_id = %s",
            (data['date'], data['text'], actual_id)
        )
        db.commit()
        
        print(f"[DEBUG] æ›´æ–°æˆåŠŸ: {cursor.rowcount} è¡Œå—å½±å“")
        
        if cursor.rowcount == 0:
            print(f"[ERROR] æ›´æ–°å¤±è´¥: æ²¡æœ‰è¡Œå—å½±å“")
            return jsonify({'error': 'Update failed: no rows affected'}), 404
        
        print(f"[SUCCESS] æ³¨é‡Šæ›´æ–°æˆåŠŸ")
        return jsonify({'success': True, 'message': 'Annotation updated'}), 200
    except Exception as e:
        print(f"[ERROR] æ•°æ®åº“æ“ä½œå¼‚å¸¸: {str(e)}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()


# --- V4.7: AIåˆ†æå†…å®¹åˆ†ç¦»å­˜å‚¨API ---
@app.route('/api/annotation/<string:annotation_id>/ai-analysis', methods=['PUT'])
@require_api_auth
def update_annotation_ai_analysis(annotation_id):
    """æ›´æ–°æ³¨é‡Šçš„AIåˆ†æå†…å®¹ï¼Œåˆ†ç¦»å­˜å‚¨åŸå§‹å†…å®¹å’ŒAIåˆ†æ"""
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[AIåˆ†æ] æ›´æ–°AIåˆ†æå†…å®¹APIè°ƒç”¨")
    print(f"[AIåˆ†æ] åŸå§‹annotation_id: '{annotation_id}'")
    print(f"[AIåˆ†æ] è§£ç åannotation_id: '{decoded_id}'")
    
    data = request.get_json()
    print(f"[AIåˆ†æ] æ¥æ”¶åˆ°çš„æ•°æ®: {data}")
    
    # å¢å¼ºæ•°æ®éªŒè¯
    if not data:
        print(f"[ERROR] è¯·æ±‚ä½“ä¸ºç©º")
        return jsonify({'error': 'Request body is empty'}), 400
        
    if 'ai_analysis' not in data:
        print(f"[ERROR] ç¼ºå°‘AIåˆ†ææ•°æ®å­—æ®µ")
        return jsonify({'error': 'Missing ai_analysis field'}), 400
    
    ai_content = data['ai_analysis']
    if not ai_content or not isinstance(ai_content, str):
        print(f"[ERROR] AIåˆ†æå†…å®¹ä¸ºç©ºæˆ–æ ¼å¼æ— æ•ˆ")
        return jsonify({'error': 'AI analysis content is empty or invalid'}), 400
    
    # å†…å®¹é•¿åº¦éªŒè¯
    if len(ai_content.strip()) < 10:
        print(f"[ERROR] AIåˆ†æå†…å®¹è¿‡çŸ­: {len(ai_content)} å­—ç¬¦")
        return jsonify({'error': 'AI analysis content too short'}), 400
    
    if len(ai_content) > 100000:  # 100KB é™åˆ¶
        print(f"[WARNING] AIåˆ†æå†…å®¹è¾ƒé•¿: {len(ai_content)} å­—ç¬¦")
        ai_content = ai_content[:100000] + "...[å†…å®¹å·²æˆªæ–­]"
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
        db_execute(cursor, "SELECT * FROM annotations WHERE annotation_id = %s OR annotation_id = %s", 
                      (annotation_id, decoded_id))
        existing = cursor.fetchone()
        
        if not existing:
            print(f"[ERROR] æ³¨é‡Šæœªæ‰¾åˆ°: {annotation_id}")
            return jsonify({'error': f'Annotation not found: {annotation_id}'}), 404
        
        # ä½¿ç”¨æŸ¥è¯¢åˆ°çš„å®é™…ID
        actual_id = existing['annotation_id']
        print(f"[AIåˆ†æ] æ‰¾åˆ°è®°å½•ï¼Œä½¿ç”¨å®é™…ID: '{actual_id}'")
        print(f"[AIåˆ†æ] åŸå§‹æ–‡æœ¬å­˜åœ¨: {bool(existing['original_text'])}")
        print(f"[AIåˆ†æ] AIåˆ†æå­˜åœ¨: {bool(existing['ai_analysis'])}")
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ·»åŠ AIåˆ†æï¼Œéœ€è¦ä¿å­˜åŸå§‹æ–‡æœ¬
        if not existing['original_text']:
            # ä¿å­˜åŸå§‹æ–‡æœ¬
            original_text = existing['text'] or ""
            print(f"[AIåˆ†æ] é¦–æ¬¡æ·»åŠ AIåˆ†æï¼Œä¿å­˜åŸå§‹æ–‡æœ¬: {len(original_text)} å­—ç¬¦")
            
            # æ„å»ºåˆå¹¶æ–‡æœ¬ï¼šAIåˆ†æåœ¨å‰ï¼Œç®—æ³•å†…å®¹åœ¨å
            combined_text = f"{ai_content}\n\n{original_text}"
            
            db_execute(cursor, """
                UPDATE annotations 
                SET original_text = %s, ai_analysis = %s, text = %s, 
                    algorithm_type = 'ai_analysis', updated_at = CURRENT_TIMESTAMP 
                WHERE annotation_id = %s
            """, (original_text, ai_content, combined_text, actual_id))
        else:
            # å¦‚æœå·²æœ‰AIåˆ†æï¼Œåªæ›´æ–°AIåˆ†æå†…å®¹
            print(f"[AIåˆ†æ] æ›´æ–°ç°æœ‰AIåˆ†æå†…å®¹")
            original_text = existing['original_text'] or ""
            combined_text = f"{ai_content}\n\n{original_text}"
            
            db_execute(cursor, """
                UPDATE annotations 
                SET ai_analysis = %s, text = %s, updated_at = CURRENT_TIMESTAMP 
                WHERE annotation_id = %s
            """, (ai_content, combined_text, actual_id))
        
        db.commit()
        
        print(f"[AIåˆ†æ] æ•°æ®åº“æ›´æ–°æˆåŠŸ: {cursor.rowcount} è¡Œå—å½±å“")
        print(f"[AIåˆ†æ] åˆå¹¶åæ–‡æœ¬é•¿åº¦: {len(combined_text)} å­—ç¬¦")
        
        if cursor.rowcount == 0:
            print(f"[ERROR] AIåˆ†ææ›´æ–°å¤±è´¥: æ²¡æœ‰è¡Œå—å½±å“")
            return jsonify({'error': 'No rows were updated'}), 500
        
        print(f"[SUCCESS] AIåˆ†æå†…å®¹æ›´æ–°æˆåŠŸ")
        return jsonify({
            'success': True, 
            'message': 'AI analysis updated successfully',
            'annotation_id': actual_id,
            'content_length': len(ai_content),
            'combined_length': len(combined_text)
        }), 200
        
    except sqlite3.Error as e:
        print(f"[ERROR] æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")
        return jsonify({'error': f'Database error: {str(e)}'}), 500
    except Exception as e:
        print(f"[ERROR] AIåˆ†æAPIå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500
    finally:
        if 'db' in locals() and db:
            db.close()


# --- V5.8: AIåˆ†æåç«¯ä»£ç†API ---
@app.route('/api/ai/dify-run', methods=['POST'])
@require_api_auth
def dify_proxy_api():
    """åç«¯ä»£ç†Dify AIå·¥ä½œæµï¼Œè§£å†³CORSå’Œå®‰å…¨é—®é¢˜"""
    import requests
    import time
    import logging
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join('logs', 'ai_analysis.log')
    os.makedirs('logs', exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s %(levelname)s %(message)s',
        filemode='a'
    )
    
    try:
        start_time = time.time()
        data = request.get_json()
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not data or 'input' not in data:
            logging.error("Difyä»£ç†è°ƒç”¨å¤±è´¥: ç¼ºå°‘inputå‚æ•°")
            return jsonify({'error': 'Missing input parameter'}), 400
        
        input_text = data['input']
        if not input_text or not isinstance(input_text, str):
            logging.error("Difyä»£ç†è°ƒç”¨å¤±è´¥: inputå‚æ•°æ— æ•ˆ")
            return jsonify({'error': 'Invalid input parameter'}), 400

        # ä»ç¯å¢ƒå˜é‡è·å–Dify API tokenï¼ˆä»…ç”Ÿäº§ç¯å¢ƒéœ€è¦ï¼‰
        dify_token = os.environ.get('DIFY_API_TOKEN')
        if not dify_token:
            logging.error("Dify API Tokenæœªé…ç½®ï¼Œè¯·è®¾ç½®DIFY_API_TOKENç¯å¢ƒå˜é‡")
            return jsonify({'error': 'DIFY_API_TOKEN not configured'}), 500

        # è®°å½•è°ƒç”¨ä¿¡æ¯
        annotation_id = data.get('annotation_id', 'unknown')
        ticker = data.get('ticker', 'unknown')
        date = data.get('date', 'unknown')

        # V5.8: è·å–AIæ¨¡å¼ï¼Œé»˜è®¤ä¸ºpro
        ai_mode = data.get('ai_mode', 'pro')  # flash/pro/ultra
        
        print(f"[Difyä»£ç†] å¼€å§‹å¤„ç†: annotation_id={annotation_id}, ticker={ticker}, date={date}, ai_mode={ai_mode}")
        print(f"[Difyä»£ç†] è¾“å…¥é•¿åº¦: {len(input_text)} å­—ç¬¦")

        # V5.8: ç›´æ¥æ„å»ºæ–°APIæ ¼å¼çš„å‚æ•°ï¼Œä¸å†éœ€è¦è·å–å‚æ•°é…ç½®
        inputs = {
            "Content": input_text,  # æ–°APIä½¿ç”¨Contentå‚æ•°
            "model": ai_mode        # æ–°å¢modelå‚æ•°
        }

        print(f"[Difyä»£ç†] æ„å»ºè¾“å…¥å‚æ•°: {list(inputs.keys())}, model={ai_mode}")
        
        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨å·¥ä½œæµï¼Œä½¿ç”¨600ç§’è¶…æ—¶
        try:
            workflow_response = requests.post(
                'https://work.pgi.chat/v1/workflows/run',
                headers={
                    'Authorization': f'Bearer {dify_token}',
                    'Content-Type': 'application/json'
                },
                json={
                    'inputs': inputs,
                    'response_mode': 'blocking',
                    'user': 'stock-analysis-system'
                },
                timeout=600  # 600ç§’è¶…æ—¶
            )
            
            if not workflow_response.ok:
                try:
                    error_data = workflow_response.json()
                    error_msg = error_data.get('message', 'å·¥ä½œæµè°ƒç”¨å¤±è´¥')
                except:
                    error_msg = f"å·¥ä½œæµè°ƒç”¨å¤±è´¥: HTTP {workflow_response.status_code}"
                
                logging.error(f"Difyå·¥ä½œæµå¤±è´¥ annotation_id={annotation_id} error={error_msg}")
                return jsonify({'error': error_msg}), 500
            
            result = workflow_response.json()
            
            # V5.7.5: æ™ºèƒ½éªŒè¯ç»“æœ - æ”¯æŒpartial-succeededçŠ¶æ€
            data = result.get('data', {})
            status = data.get('status', '')
            outputs = data.get('outputs', {})

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æˆåŠŸçŠ¶æ€
            valid_success_statuses = ['succeeded', 'partial-succeeded']
            if not data or status not in valid_success_statuses:
                error_msg = f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥æˆ–æœªæˆåŠŸå®Œæˆï¼ŒçŠ¶æ€: {status}"
                logging.error(f"Difyå·¥ä½œæµçŠ¶æ€å¼‚å¸¸ annotation_id={annotation_id} status={status}")
                return jsonify({'error': error_msg}), 500

            # å³ä½¿æ˜¯partial-succeededï¼Œä¹Ÿè¦ç¡®ä¿æœ‰æœ‰æ•ˆè¾“å‡º
            if not outputs:
                error_msg = f"å·¥ä½œæµçŠ¶æ€ä¸º{status}ä½†è¿”å›ç©ºç»“æœ"
                logging.error(f"Difyå·¥ä½œæµç©ºç»“æœ annotation_id={annotation_id} status={status}")
                return jsonify({'error': error_msg}), 500

            # å¦‚æœæ˜¯partial-succeededï¼Œè®°å½•ä½†ç»§ç»­å¤„ç†
            if status == 'partial-succeeded':
                logging.warning(f"Difyå·¥ä½œæµéƒ¨åˆ†æˆåŠŸ annotation_id={annotation_id}ï¼Œä½†æœ‰æœ‰æ•ˆè¾“å‡ºï¼Œç»§ç»­å¤„ç†")
            
            # è®¡ç®—è€—æ—¶
            duration = time.time() - start_time
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            logging.info(f"Difyåˆ†ææˆåŠŸ annotation_id={annotation_id} ticker={ticker} date={date} input_length={len(input_text)} duration={duration:.2f}s")
            print(f"[Difyä»£ç†] åˆ†ææˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’")
            
            return jsonify({
                'success': True,
                'data': outputs,
                'duration': duration,
                'input_length': len(input_text)
            }), 200
            
        except requests.exceptions.Timeout:
            error_msg = "AIåˆ†æè¶…æ—¶(600ç§’)ï¼Œè¯·ç¨åé‡è¯•"
            logging.error(f"Difyåˆ†æè¶…æ—¶ annotation_id={annotation_id} ticker={ticker} date={date}")
            return jsonify({'error': error_msg}), 408
            
        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
            logging.error(f"Difyç½‘ç»œé”™è¯¯ annotation_id={annotation_id} error={error_msg}")
            return jsonify({'error': error_msg}), 500
        
    except Exception as e:
        error_msg = f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        logging.error(f"Difyä»£ç†å¼‚å¸¸ error={error_msg}")
        print(f"[ERROR] Difyä»£ç†APIå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': error_msg}), 500


# --- V5.8: å¼‚æ­¥AIåˆ†æä»»åŠ¡ç®¡ç† ---
import threading
import uuid
import datetime as dt

# å†…å­˜ä¸­çš„ä»»åŠ¡çŠ¶æ€å­˜å‚¨ (ç”Ÿäº§ç¯å¢ƒå¯ä»¥ä½¿ç”¨Redis)
ai_tasks = {}
task_lock = threading.Lock()

def cleanup_old_tasks():
    """æ¸…ç†24å°æ—¶å‰çš„æ—§ä»»åŠ¡"""
    cutoff_time = dt.datetime.now() - dt.timedelta(hours=24)
    with task_lock:
        expired_tasks = [task_id for task_id, task in ai_tasks.items() 
                        if task.get('created_at', dt.datetime.now()) < cutoff_time]
        for task_id in expired_tasks:
            del ai_tasks[task_id]
    
def background_ai_analysis(task_id, annotation_id, input_text, ticker, date, ai_mode='pro'):
    """åå°æ‰§è¡ŒAIåˆ†æçš„å‡½æ•° (V5.8: æ·»åŠ AIæ¨¡å¼æ”¯æŒ)"""
    # é…ç½®æ—¥å¿—è®°å½•å™¨
    log_file = os.path.join('logs', 'ai_analysis.log')
    os.makedirs('logs', exist_ok=True)
    import logging

    # åˆ›å»ºç‹¬ç«‹çš„loggeré¿å…å†²çª
    logger = logging.getLogger(f'ai_analysis_{task_id}')
    logger.setLevel(logging.INFO)

    # é¿å…é‡å¤æ·»åŠ handler
    if not logger.handlers:
        handler = logging.FileHandler(log_file, mode='a')
        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    try:
        logger.info(f"[ASYNC-{task_id}] å¼€å§‹å¼‚æ­¥AIåˆ†æ: annotation_id={annotation_id}, ticker={ticker}, date={date}, input_length={len(input_text)}, ai_mode={ai_mode}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
        with task_lock:
            if task_id in ai_tasks:
                ai_tasks[task_id]['status'] = 'processing'
                ai_tasks[task_id]['updated_at'] = dt.datetime.now()
                logger.info(f"[ASYNC-{task_id}] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º processing")

        # æ‰§è¡ŒåŸæœ‰çš„Difyè°ƒç”¨é€»è¾‘
        import requests
        import time
        from requests.exceptions import ConnectTimeout, ReadTimeout, Timeout

        start_time = time.time()

        # ä»ç¯å¢ƒå˜é‡è·å–Dify API token
        dify_token = os.environ.get('DIFY_API_TOKEN')
        if not dify_token:
            logger.error(f"[ASYNC-{task_id}] Dify API Tokenæœªé…ç½®")
            with task_lock:
                ai_tasks[task_id]['status'] = 'failed'
                ai_tasks[task_id]['error'] = 'DIFY_API_TOKEN not configured'
            return
        logger.info(f"[ASYNC-{task_id}] ä½¿ç”¨ Dify API Token: {dify_token[:10]}...")

        # V5.8: ç›´æ¥æ„å»ºæ–°APIæ ¼å¼çš„å‚æ•°ï¼Œä¸å†éœ€è¦è·å–å‚æ•°é…ç½®
        inputs = {
            "Content": input_text,  # æ–°APIä½¿ç”¨Contentå‚æ•°
            "model": ai_mode        # æ–°å¢modelå‚æ•°
        }

        logger.info(f"[ASYNC-{task_id}] æ„å»ºè¾“å…¥å‚æ•°: {list(inputs.keys())}, model={ai_mode}")

        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨å·¥ä½œæµ
        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: å¼€å§‹è°ƒç”¨Difyå·¥ä½œæµ")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ - åŠ ä¸€ä¸ªè°ƒç”¨ä¸­çš„çŠ¶æ€
        with task_lock:
            if task_id in ai_tasks:
                ai_tasks[task_id]['progress'] = 'calling_dify_api'
                ai_tasks[task_id]['updated_at'] = dt.datetime.now()

        try:
            workflow_response = requests.post(
                'https://work.pgi.chat/v1/workflows/run',
                headers={
                    'Authorization': f'Bearer {dify_token}',
                    'Content-Type': 'application/json'
                },
                json={
                    'inputs': inputs,
                    'response_mode': 'blocking',
                    'user': 'stock-analysis-system'
                },
                timeout=600  # 600ç§’è¶…æ—¶
            )

            logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: å·¥ä½œæµè°ƒç”¨å®Œæˆï¼ŒçŠ¶æ€ç : {workflow_response.status_code}")

            # V5.7.3: æ™ºèƒ½504é”™è¯¯å¤„ç† - Difyç½‘å…³è¶…æ—¶ä½†å¯èƒ½å·²å®Œæˆå¤„ç†
            if workflow_response.status_code == 504:
                logger.warning(f"[ASYNC-{task_id}] æ­¥éª¤2: æ”¶åˆ°504ç½‘å…³è¶…æ—¶ï¼ŒDifyå¯èƒ½å·²å®Œæˆå¤„ç†ï¼Œå¯åŠ¨æ™ºèƒ½é‡è¯•")

                # å…ˆå°è¯•è§£æå½“å‰å“åº”
                try:
                    result = workflow_response.json()
                    if result.get('data') and result['data'].get('status') == 'succeeded':
                        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: 504çŠ¶æ€ä½†å“åº”åŒ…å«æˆåŠŸç»“æœï¼Œç›´æ¥ä½¿ç”¨")
                    else:
                        raise ValueError("504å“åº”æ— æœ‰æ•ˆç»“æœ")
                except (json.JSONDecodeError, ValueError):
                    # æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼šç”¨æ›´ç®€å•çš„è¯·æ±‚æ£€æŸ¥Difyæ˜¯å¦å·²å®Œæˆ
                    logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: 504å“åº”æ— æ³•è§£æï¼Œå¯åŠ¨æ™ºèƒ½é‡è¯•æœºåˆ¶")

                    max_retries = 3
                    retry_success = False

                    for retry_count in range(max_retries):
                        wait_time = 3 + retry_count * 2  # 3, 5, 7ç§’é€’å¢ç­‰å¾…
                        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: æ™ºèƒ½é‡è¯• {retry_count + 1}/{max_retries}ï¼Œç­‰å¾…{wait_time}ç§’...")
                        time.sleep(wait_time)

                        try:
                            # ä½¿ç”¨ç›¸åŒå‚æ•°ä½†æ›´çŸ­è¶…æ—¶é‡è¯•
                            retry_response = requests.post(
                                'https://work.pgi.chat/v1/workflows/run',
                                headers={
                                    'Authorization': f'Bearer {dify_token}',
                                    'Content-Type': 'application/json'
                                },
                                json={
                                    'inputs': inputs,
                                    'response_mode': 'blocking',
                                    'user': 'stock-analysis-system'
                                },
                                timeout=60  # çŸ­è¶…æ—¶ï¼Œå¦‚æœå·²å®Œæˆåº”è¯¥å¾ˆå¿«å“åº”
                            )

                            logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: é‡è¯•å“åº”çŠ¶æ€ç : {retry_response.status_code}")

                            if retry_response.status_code == 200:
                                logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: ğŸ‰ æ™ºèƒ½é‡è¯•æˆåŠŸ!")
                                workflow_response = retry_response
                                retry_success = True
                                break
                            elif retry_response.status_code != 504:
                                # é504é”™è¯¯ï¼Œè¯´æ˜æœ‰å…¶ä»–é—®é¢˜ï¼Œåœæ­¢é‡è¯•
                                logger.warning(f"[ASYNC-{task_id}] æ­¥éª¤2: é‡è¯•é‡åˆ°æ–°é”™è¯¯ {retry_response.status_code}ï¼Œåœæ­¢é‡è¯•")
                                workflow_response = retry_response
                                break

                        except Exception as retry_error:
                            logger.warning(f"[ASYNC-{task_id}] æ­¥éª¤2: é‡è¯• {retry_count + 1} å¼‚å¸¸: {str(retry_error)[:100]}")

                    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
                    if not retry_success and workflow_response.status_code == 504:
                        logger.error(f"[ASYNC-{task_id}] æ­¥éª¤2: æ‰€æœ‰æ™ºèƒ½é‡è¯•å‡å¤±è´¥ï¼Œåˆ¤å®šä¸ºçœŸæ­£çš„è¶…æ—¶å¤±è´¥")
                        raise Exception(f"Dify APIç½‘å…³è¶…æ—¶ (å·²é‡è¯•{max_retries}æ¬¡): è¯·æ±‚å¯èƒ½è¿‡äºå¤æ‚æˆ–æœåŠ¡ç¹å¿™")

            elif not workflow_response.ok:
                try:
                    error_data = workflow_response.json()
                    error_msg = error_data.get('message', 'å·¥ä½œæµè°ƒç”¨å¤±è´¥')
                except:
                    error_msg = f"å·¥ä½œæµè°ƒç”¨å¤±è´¥: HTTP {workflow_response.status_code}"
                raise Exception(error_msg)
            else:
                result = workflow_response.json()

        except (ConnectTimeout, ReadTimeout, Timeout) as timeout_error:
            logger.error(f"[ASYNC-{task_id}] æ­¥éª¤2: Dify APIè°ƒç”¨è¶…æ—¶: {str(timeout_error)}")
            # å¯¹äºè¶…æ—¶é”™è¯¯ï¼Œæˆ‘ä»¬å…ˆç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç„¶åå°è¯•é€šè¿‡å…¶ä»–æ–¹å¼ç¡®è®¤æ˜¯å¦æˆåŠŸ
            logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: è¶…æ—¶åç­‰å¾…5ç§’ï¼Œç„¶åæ ‡è®°ä¸ºå¤±è´¥")
            time.sleep(5)
            raise Exception(f"Dify APIè°ƒç”¨è¶…æ—¶: {str(timeout_error)}")

        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: å¼€å§‹éªŒè¯å·¥ä½œæµç»“æœ")

        # V5.7.5: æ™ºèƒ½éªŒè¯ç»“æœ - æ”¯æŒpartial-succeededçŠ¶æ€
        data = result.get('data', {})
        status = data.get('status', '')
        outputs = data.get('outputs', {})

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æˆåŠŸçŠ¶æ€
        valid_success_statuses = ['succeeded', 'partial-succeeded']
        if not data or status not in valid_success_statuses:
            error_msg = f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥æˆ–æœªæˆåŠŸå®Œæˆ: status={status}, data={data}"
            logger.error(f"[ASYNC-{task_id}] æ­¥éª¤2: {error_msg}")
            raise Exception(error_msg)

        # å³ä½¿æ˜¯partial-succeededï¼Œä¹Ÿè¦ç¡®ä¿æœ‰æœ‰æ•ˆè¾“å‡º
        if not outputs:
            error_msg = f"å·¥ä½œæµçŠ¶æ€ä¸º{status}ä½†è¿”å›ç©ºç»“æœ"
            logger.error(f"[ASYNC-{task_id}] æ­¥éª¤2: {error_msg}")
            raise Exception(error_msg)

        # å¦‚æœæ˜¯partial-succeededï¼Œè®°å½•ä½†ç»§ç»­å¤„ç†
        if status == 'partial-succeeded':
            logger.warning(f"[ASYNC-{task_id}] æ­¥éª¤2: Difyå·¥ä½œæµéƒ¨åˆ†æˆåŠŸï¼Œä½†æœ‰æœ‰æ•ˆè¾“å‡ºï¼Œç»§ç»­å¤„ç†")
        else:
            logger.info(f"[ASYNC-{task_id}] æ­¥éª¤2: Difyå·¥ä½œæµå®Œå…¨æˆåŠŸ")

        duration = time.time() - start_time
        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤3: Difyåˆ†ææˆåŠŸå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œç»“æœé•¿åº¦: {len(str(outputs))}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
        logger.info(f"[ASYNC-{task_id}] æ­¥éª¤4: å¼€å§‹æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºcompleted")
        with task_lock:
            if task_id in ai_tasks:
                ai_tasks[task_id].update({
                    'status': 'completed',
                    'result': outputs,
                    'duration': duration,
                    'updated_at': dt.datetime.now(),
                    'progress': 'completed'
                })
                logger.info(f"[ASYNC-{task_id}] æ­¥éª¤4: ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸï¼Œstatus=completed")
            else:
                logger.error(f"[ASYNC-{task_id}] æ­¥éª¤4: è­¦å‘Š - ä»»åŠ¡IDåœ¨ai_tasksä¸­ä¸å­˜åœ¨")

        logger.info(f"[ASYNC-{task_id}] å¼‚æ­¥AIåˆ†æå®Œå…¨æˆåŠŸ: annotation_id={annotation_id}, ticker={ticker}, date={date}, duration={duration:.2f}s")

    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        logger.error(f"[ASYNC-{task_id}] å¼‚æ­¥AIåˆ†æå¤±è´¥: error_type={error_type}, error={error_msg}")
        print(f"[åå°AIåˆ†æ] ä»»åŠ¡ {task_id} å¤±è´¥: {error_msg}")

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        with task_lock:
            if task_id in ai_tasks:
                ai_tasks[task_id].update({
                    'status': 'failed',
                    'error': error_msg,
                    'error_type': error_type,
                    'updated_at': dt.datetime.now(),
                    'progress': 'failed'
                })
                logger.info(f"[ASYNC-{task_id}] ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºfailed")
            else:
                logger.error(f"[ASYNC-{task_id}] è­¦å‘Š - æ— æ³•æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œä»»åŠ¡IDä¸å­˜åœ¨")

@app.route('/api/ai/dify-async', methods=['POST'])
@require_api_auth  
def dify_async_start():
    """å¯åŠ¨å¼‚æ­¥AIåˆ†æä»»åŠ¡"""
    try:
        data = request.get_json()
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not data or 'input' not in data:
            return jsonify({'error': 'Missing input parameter'}), 400
        
        input_text = data['input']
        if not input_text or not isinstance(input_text, str):
            return jsonify({'error': 'Invalid input parameter'}), 400
        
        # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        annotation_id = data.get('annotation_id', 'unknown')
        ticker = data.get('ticker', 'unknown')
        date = data.get('date', 'unknown')

        # V5.8: è·å–AIæ¨¡å¼ï¼Œé»˜è®¤ä¸ºpro
        ai_mode = data.get('ai_mode', 'pro')  # flash/pro/ultra
        
        # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        with task_lock:
            ai_tasks[task_id] = {
                'annotation_id': annotation_id,
                'ticker': ticker,
                'date': date,
                'status': 'pending',
                'created_at': dt.datetime.now(),
                'updated_at': dt.datetime.now(),
                'input_length': len(input_text)
            }
        
        # å¯åŠ¨åå°çº¿ç¨‹å¤„ç†AIåˆ†æ
        thread = threading.Thread(
            target=background_ai_analysis,
            args=(task_id, annotation_id, input_text, ticker, date, ai_mode)
        )
        thread.daemon = True
        thread.start()

        print(f"[å¼‚æ­¥AIåˆ†æ] ä»»åŠ¡ {task_id} å·²å¯åŠ¨: annotation_id={annotation_id}, ticker={ticker}, ai_mode={ai_mode}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'status': 'pending',
            'message': 'AIåˆ†æä»»åŠ¡å·²å¯åŠ¨'
        }), 200
        
    except Exception as e:
        error_msg = f"å¯åŠ¨å¼‚æ­¥ä»»åŠ¡å¤±è´¥: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return jsonify({'error': error_msg}), 500

@app.route('/api/ai/task/<string:task_id>', methods=['GET'])
@require_api_auth
def get_ai_task_status(task_id):
    """è·å–AIåˆ†æä»»åŠ¡çŠ¶æ€"""
    try:
        # æ¸…ç†æ—§ä»»åŠ¡
        cleanup_old_tasks()

        with task_lock:
            if task_id not in ai_tasks:
                return jsonify({'error': 'Task not found'}), 404

            task = ai_tasks[task_id].copy()

        # è½¬æ¢datetimeå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
        task['created_at'] = task['created_at'].isoformat()
        task['updated_at'] = task['updated_at'].isoformat()

        # è®¡ç®—ä»»åŠ¡è¿è¡Œæ—¶é—´
        created_time = dt.datetime.fromisoformat(task['created_at'])
        current_time = dt.datetime.now()
        running_time = (current_time - created_time).total_seconds()
        task['running_time'] = round(running_time, 2)

        # æ·»åŠ çŠ¶æ€æè¿°
        status_descriptions = {
            'pending': 'ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¼€å§‹',
            'processing': 'æ­£åœ¨åˆ†æä¸­...',
            'completed': 'AIåˆ†æå·²å®Œæˆ',
            'failed': 'AIåˆ†æå¤±è´¥'
        }

        progress_descriptions = {
            'calling_dify_api': 'æ­£åœ¨è°ƒç”¨Dify API...',
            'completed': 'åˆ†æå®Œæˆ',
            'failed': 'åˆ†æå¤±è´¥'
        }

        task['status_description'] = status_descriptions.get(task['status'], task['status'])
        if 'progress' in task:
            task['progress_description'] = progress_descriptions.get(task['progress'], task['progress'])

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨éç”Ÿäº§ç¯å¢ƒï¼‰
        if not os.environ.get('DATABASE_URL'):  # æœ¬åœ°å¼€å‘ç¯å¢ƒ
            task['debug_info'] = {
                'task_exists_in_memory': True,
                'task_keys': list(task.keys()),
                'total_tasks_in_memory': len(ai_tasks)
            }

        return jsonify({
            'success': True,
            'task_id': task_id,
            'task': task
        }), 200

    except Exception as e:
        error_msg = f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return jsonify({'error': error_msg}), 500

@app.route('/api/ai/tasks/status', methods=['GET'])
@require_api_auth
def get_all_tasks_status():
    """è·å–æ‰€æœ‰AIåˆ†æä»»åŠ¡çš„çŠ¶æ€æ¦‚è§ˆ"""
    try:
        cleanup_old_tasks()

        with task_lock:
            all_tasks = {}
            for task_id, task in ai_tasks.items():
                task_copy = task.copy()
                # è½¬æ¢datetimeå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
                task_copy['created_at'] = task_copy['created_at'].isoformat()
                task_copy['updated_at'] = task_copy['updated_at'].isoformat()

                # è®¡ç®—è¿è¡Œæ—¶é—´
                created_time = dt.datetime.fromisoformat(task_copy['created_at'])
                current_time = dt.datetime.now()
                running_time = (current_time - created_time).total_seconds()
                task_copy['running_time'] = round(running_time, 2)

                all_tasks[task_id] = task_copy

        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„ä»»åŠ¡æ•°é‡
        status_stats = {
            'pending': 0,
            'processing': 0,
            'completed': 0,
            'failed': 0,
            'total': len(all_tasks)
        }

        failed_tasks = []
        long_running_tasks = []

        for task_id, task in all_tasks.items():
            status_stats[task['status']] += 1

            # æ”¶é›†å¤±è´¥çš„ä»»åŠ¡
            if task['status'] == 'failed':
                failed_tasks.append({
                    'task_id': task_id,
                    'annotation_id': task.get('annotation_id'),
                    'ticker': task.get('ticker'),
                    'date': task.get('date'),
                    'error': task.get('error'),
                    'error_type': task.get('error_type'),
                    'running_time': task['running_time']
                })

            # æ”¶é›†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼ˆè¶…è¿‡10åˆ†é’Ÿï¼‰
            if task['status'] in ['pending', 'processing'] and task['running_time'] > 600:
                long_running_tasks.append({
                    'task_id': task_id,
                    'annotation_id': task.get('annotation_id'),
                    'ticker': task.get('ticker'),
                    'date': task.get('date'),
                    'status': task['status'],
                    'running_time': task['running_time']
                })

        return jsonify({
            'success': True,
            'stats': status_stats,
            'failed_tasks': failed_tasks,
            'long_running_tasks': long_running_tasks,
            'all_tasks': all_tasks if not os.environ.get('DATABASE_URL') else {}  # ä»…åœ¨å¼€å‘ç¯å¢ƒè¿”å›å…¨éƒ¨ä»»åŠ¡
        }), 200

    except Exception as e:
        error_msg = f"è·å–ä»»åŠ¡çŠ¶æ€æ¦‚è§ˆå¤±è´¥: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return jsonify({'error': error_msg}), 500

@app.route('/api/ai/task/<string:task_id>/retry', methods=['POST'])
@require_api_auth
def retry_ai_task(task_id):
    """é‡æ–°å°è¯•ä¸€ä¸ªå¤±è´¥çš„AIåˆ†æä»»åŠ¡"""
    try:
        with task_lock:
            if task_id not in ai_tasks:
                return jsonify({'error': 'Task not found'}), 404

            task = ai_tasks[task_id]

            # åªå…è®¸é‡è¯•å¤±è´¥çš„ä»»åŠ¡
            if task['status'] != 'failed':
                return jsonify({'error': f'Cannot retry task with status: {task["status"]}'}), 400

            # è·å–åŸå§‹ä»»åŠ¡ä¿¡æ¯
            annotation_id = task.get('annotation_id', 'unknown')
            ticker = task.get('ticker', 'unknown')
            date = task.get('date', 'unknown')

            # æˆ‘ä»¬éœ€è¦é‡æ–°è·å–è¾“å…¥æ–‡æœ¬ï¼Œè¿™é‡Œå…ˆè¿”å›é”™è¯¯æç¤º
            return jsonify({
                'error': 'Task retry not implemented yet. Please use the normal AI analysis button to restart the analysis.',
                'suggestion': 'è¯·åœ¨æ³¨é‡Šåˆ—è¡¨ä¸­ç‚¹å‡»"è‡ªåŠ¨åˆ†æ"æŒ‰é’®é‡æ–°å¼€å§‹åˆ†æ',
                'task_info': {
                    'annotation_id': annotation_id,
                    'ticker': ticker,
                    'date': date
                }
            }), 501  # Not Implemented

    except Exception as e:
        error_msg = f"é‡è¯•ä»»åŠ¡å¤±è´¥: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return jsonify({'error': error_msg}), 500


# --- V3.7: å›æ”¶ç«™API ---
@app.route('/api/recycle/annotations')
@require_api_auth
def get_deleted_annotations():
    """è·å–å›æ”¶ç«™ä¸­çš„å·²åˆ é™¤æ³¨é‡Š"""
    ticker = request.args.get('ticker', '')
    if not ticker:
        return jsonify({'error': 'Ticker parameter required'}), 400
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æ ‡å‡†åŒ–tickeræŸ¥è¯¢
        normalized_ticker, _ = normalize_ticker(ticker)
        
        # è·å–æŒ‡å®šè‚¡ç¥¨çš„å·²åˆ é™¤æ³¨é‡Š
        db_execute(cursor, """
            SELECT annotation_id, ticker, date, text, annotation_type, algorithm_type, 
                   is_favorite, deleted_at, created_at
            FROM annotations 
            WHERE ticker = %s AND is_deleted = 1
            ORDER BY deleted_at DESC
        """, (normalized_ticker,))
        
        deleted_rows = cursor.fetchall()
        deleted_annotations = [
            {
                'id': row['annotation_id'],
                'ticker': row['ticker'],
                'date': row['date'], 
                'text': row['text'],
                'type': row['annotation_type'],
                'algorithm_type': row['algorithm_type'],
                'is_favorite': bool(row['is_favorite']) if row['is_favorite'] is not None else False,
                'deleted_at': row['deleted_at'],
                'created_at': row['created_at']
            }
            for row in deleted_rows
        ]
        
        cursor.close()
        db.close()
        
        return jsonify({
            'success': True,
            'deleted_annotations': deleted_annotations,
            'count': len(deleted_annotations)
        }), 200
        
    except Exception as e:
        print(f"[ERROR] è·å–å›æ”¶ç«™æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/recycle/restore/<string:annotation_id>', methods=['POST'])
@require_api_auth
def restore_annotation(annotation_id):
    """ä»å›æ”¶ç«™æ¢å¤æ³¨é‡Š"""
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] æ¢å¤æ³¨é‡ŠAPIè°ƒç”¨: {decoded_id}")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æ£€æŸ¥æ³¨é‡Šæ˜¯å¦åœ¨å›æ”¶ç«™ä¸­
        db_execute(cursor, """
            SELECT annotation_id FROM annotations 
            WHERE (annotation_id = %s OR annotation_id = %s) AND is_deleted = 1
        """, (annotation_id, decoded_id))
        
        existing = cursor.fetchone()
        if not existing:
            return jsonify({'error': 'Annotation not found in recycle bin'}), 404
        
        actual_id = existing['annotation_id']
        
        # æ¢å¤æ³¨é‡Šï¼šè®¾ç½® is_deleted = 0ï¼Œæ¸…ç©º deleted_at
        db_execute(cursor, """
            UPDATE annotations 
            SET is_deleted = 0, deleted_at = NULL, updated_at = CURRENT_TIMESTAMP
            WHERE annotation_id = %s
        """, (actual_id,))
        
        db.commit()
        
        print(f"[DEBUG] æ³¨é‡Šæ¢å¤æˆåŠŸ: {actual_id}")
        return jsonify({'success': True, 'message': 'Annotation restored'}), 200
        
    except Exception as e:
        print(f"[ERROR] æ¢å¤æ³¨é‡Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()

@app.route('/api/recycle/permanent-delete/<string:annotation_id>', methods=['DELETE'])
@require_api_auth
def permanent_delete_annotation(annotation_id):
    """æ°¸ä¹…åˆ é™¤æ³¨é‡Šï¼ˆä»å›æ”¶ç«™å½»åº•åˆ é™¤ï¼‰"""
    import urllib.parse
    decoded_id = urllib.parse.unquote(annotation_id)
    print(f"[DEBUG] æ°¸ä¹…åˆ é™¤æ³¨é‡ŠAPIè°ƒç”¨: {decoded_id}")
    
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æ£€æŸ¥æ³¨é‡Šæ˜¯å¦åœ¨å›æ”¶ç«™ä¸­
        db_execute(cursor, """
            SELECT annotation_id FROM annotations 
            WHERE (annotation_id = %s OR annotation_id = %s) AND is_deleted = 1
        """, (annotation_id, decoded_id))
        
        existing = cursor.fetchone()
        if not existing:
            return jsonify({'error': 'Annotation not found in recycle bin'}), 404
        
        actual_id = existing['annotation_id']
        
        # æ°¸ä¹…åˆ é™¤
        db_execute(cursor, "DELETE FROM annotations WHERE annotation_id = %s", (actual_id,))
        db.commit()
        
        print(f"[DEBUG] æ³¨é‡Šæ°¸ä¹…åˆ é™¤æˆåŠŸ: {actual_id}")
        return jsonify({'success': True, 'message': 'Annotation permanently deleted'}), 200
        
    except Exception as e:
        print(f"[ERROR] æ°¸ä¹…åˆ é™¤æ³¨é‡Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500
    finally:
        if 'db' in locals() and db:
            db.close()


@app.route('/api/annotations/export')
@require_api_auth
def export_annotations():
    """å¯¼å‡ºæŒ‡å®šæ—¶é—´æ®µçš„è‚¡ä»·å¼‚å¸¸æ ‡æ³¨æ•°æ® - æ”¯æŒåŠ¨æ€ç®—æ³•å‚æ•°"""
    try:
        # è·å–åŸºæœ¬å‚æ•°
        ticker = request.args.get('ticker')
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        if not ticker or not start_date or not end_date:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
            
        # è·å–ç®—æ³•å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        price_std_multiplier = float(request.args.get('price_std', 1.8))
        volume_std_multiplier = float(request.args.get('volume_std', 1.8))
        price_only_std_multiplier = float(request.args.get('price_only_std', 2.5))
        volume_only_std_multiplier = float(request.args.get('volume_only_std', 3.0))
        
        # ZIGæŒ‡æ ‡å‚æ•°
        short_term_zig_threshold = float(request.args.get('short_term_zig', 10))
        medium_term_zig_threshold = float(request.args.get('medium_term_zig', 10))
        long_term_zig_threshold = float(request.args.get('long_term_zig', 25))
        zig_phase_source = request.args.get('zig_phase_source', 'zig50')
        
        # æˆäº¤é‡ZIGæŒ‡æ ‡å‚æ•°
        volume_short_term_zig_threshold = float(request.args.get('volume_short_term_zig', 10))
        volume_medium_term_zig_threshold = float(request.args.get('volume_medium_term_zig', 10))
        volume_long_term_zig_threshold = float(request.args.get('volume_long_term_zig', 10))
        volume_zig_phase_source = request.args.get('volume_zig_phase_source', 'volume_zig50')
            
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        normalized_ticker, _ = normalize_ticker(ticker)
        
        # è·å–å…¬å¸åç§°
        company_name = get_company_name(normalized_ticker)
        company_info = f"{ticker} {company_name}" if company_name else ticker
        
        # æ„å»ºstock_data APIçš„URLï¼Œä½¿ç”¨å½“å‰ç®—æ³•å‚æ•°
        stock_api_params = {
            'ticker': ticker,
            'period': '1d',  # ä½¿ç”¨æ—¥çº¿æ•°æ®
            'price_std': price_std_multiplier,
            'volume_std': volume_std_multiplier,
            'price_only_std': price_only_std_multiplier,
            'volume_only_std': volume_only_std_multiplier,
            'short_term_zig': short_term_zig_threshold,
            'medium_term_zig': medium_term_zig_threshold,
            'long_term_zig': long_term_zig_threshold,
            'zig_phase_source': zig_phase_source,
            'volume_short_term_zig': volume_short_term_zig_threshold,
            'volume_medium_term_zig': volume_medium_term_zig_threshold,
            'volume_long_term_zig': volume_long_term_zig_threshold,
            'volume_zig_phase_source': volume_zig_phase_source
        }
        
        # å†…éƒ¨è°ƒç”¨stock_data APIè·å–å¸¦æœ‰å½“å‰å‚æ•°çš„å®Œæ•´æ•°æ®
        with app.test_request_context('/api/stock_data', query_string=stock_api_params):
            stock_response = stock_data()
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©ºæˆ–é”™è¯¯
            if stock_response is None:
                return jsonify({'error': 'æ— æ³•è·å–è‚¡ç¥¨æ•°æ®'}), 500
                
            if isinstance(stock_response, tuple) and stock_response[1] != 200:
                return stock_response
                
            stock_result = stock_response.get_json() if hasattr(stock_response, 'get_json') else stock_response
            
            # è¿›ä¸€æ­¥æ£€æŸ¥è‚¡ç¥¨ç»“æœæ˜¯å¦ä¸ºç©º
            if stock_result is None:
                return jsonify({'error': 'è‚¡ç¥¨æ•°æ®ä¸ºç©º'}), 500
            
        # ä»stock_dataç»“æœä¸­æå–annotations
        all_annotations = stock_result.get('annotations', [])
        
        # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µå†…çš„æ ‡æ³¨
        filtered_annotations = [
            annotation for annotation in all_annotations
            if start_date <= annotation['date'] <= end_date
        ]
        
        # è½¬æ¢æ ¼å¼å¹¶æ·»åŠ å…¬å¸ä¿¡æ¯
        annotations = []
        for annotation in filtered_annotations:
            annotations.append({
                'å…¬å¸ä¿¡æ¯': company_info,
                'date': annotation['date'],
                'text': annotation['text'],
                'type': annotation['type']
            })
        
        return jsonify({
            'success': True,
            'data': annotations,
            'count': len(annotations),
            'period': f"{start_date} è‡³ {end_date}",
            'ticker': ticker
        })
        
    except Exception as e:
        print(f"å¯¼å‡ºæ³¨é‡Šæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return jsonify({'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500
    finally:
        if 'db' in locals() and db:
            db.close()


@app.route('/api/stock_data')
@require_api_auth
def stock_data():
    import datetime as dt
    # --- è·å–å‰ç«¯å‚æ•° ---
    user_input_ticker = request.args.get('ticker', 'AAPL')
    period_param = request.args.get('period', '1d')
    
    # --- æ™ºèƒ½è‚¡ç¥¨ä»£ç è¯†åˆ«ä¸è½¬æ¢ ---
    print(f"[API] ç”¨æˆ·è¾“å…¥: {user_input_ticker}")
    
    # Step 1: å°†ç”¨æˆ·è¾“å…¥æ ‡å‡†åŒ–ä¸ºå†…éƒ¨æ ¼å¼
    normalized_ticker, identification_type = normalize_ticker(user_input_ticker)
    
    if not normalized_ticker:
        smart_error_msg = generate_smart_error_message(user_input_ticker, identification_type)
        return jsonify({'error': smart_error_msg}), 400
    
    print(f"[API] æ ‡å‡†åŒ–ç»“æœ: {user_input_ticker} -> {normalized_ticker} (ç±»å‹: {identification_type})")
    
    # Step 2: ä¸ºYahoo APIå‡†å¤‡æ­£ç¡®æ ¼å¼
    yahoo_ticker = to_yahoo_format(normalized_ticker)
    
    # Step 3: è®¾ç½®å†…éƒ¨ä½¿ç”¨çš„tickerï¼ˆç”¨äºç¼“å­˜å’Œæ˜¾ç¤ºï¼‰
    ticker = normalized_ticker
    print(f"[API] æœ€ç»ˆä½¿ç”¨ - å†…éƒ¨æ ¼å¼: {ticker}, Yahooæ ¼å¼: {yahoo_ticker}")
    
    # V1.2 & V1.8 æ–°å¢ï¼šä»å‰ç«¯è·å–ç®—æ³•å‚æ•°ï¼Œå¹¶æä¾›é»˜è®¤å€¼
    price_std_multiplier = float(request.args.get('price_std', 1.8))
    volume_std_multiplier = float(request.args.get('volume_std', 1.8))
    price_only_std_multiplier = float(request.args.get('price_only_std', 2.5))
    volume_only_std_multiplier = float(request.args.get('volume_only_std', 3.0)) # æ–°å¢ï¼šä»…æˆäº¤é‡å¼‚å¸¸çš„å€æ•°

    # ZIGæŒ‡æ ‡å‚æ•°
    short_term_zig_threshold = float(request.args.get('short_term_zig', 10))
    medium_term_zig_threshold = float(request.args.get('medium_term_zig', 10))
    long_term_zig_threshold = float(request.args.get('long_term_zig', 25))
    zig_phase_source = request.args.get('zig_phase_source', 'zig50') # æ–°å¢ï¼šç”¨äºåˆ¤æ–­åŒºé—´çš„ZIGæ¥æº

    # V2.0 æ–°å¢: æˆäº¤é‡ZIGæŒ‡æ ‡å‚æ•°
    volume_short_term_zig_threshold = float(request.args.get('volume_short_term_zig', 10))
    volume_medium_term_zig_threshold = float(request.args.get('volume_medium_term_zig', 10))
    volume_long_term_zig_threshold = float(request.args.get('volume_long_term_zig', 10))
    volume_zig_phase_source = request.args.get('volume_zig_phase_source', 'volume_zig50')

    print(f"è·å–è‚¡ç¥¨æ•°æ®: {ticker}, å‘¨æœŸ: {period_param}")
    print(f"ç®—æ³•å‚æ•°: price_std={price_std_multiplier}, volume_std={volume_std_multiplier}, price_only_std={price_only_std_multiplier}, volume_only_std={volume_only_std_multiplier}")
    print(f"ZIGå‚æ•°: short={short_term_zig_threshold}%, medium={medium_term_zig_threshold}%, long={long_term_zig_threshold}% Phase Source: {zig_phase_source}")
    print(f"æˆäº¤é‡ZIGå‚æ•°: short={volume_short_term_zig_threshold}%, medium={volume_medium_term_zig_threshold}%, long={volume_long_term_zig_threshold}% Phase Source: {volume_zig_phase_source}")

    # è·å–å…¬å¸åç§°
    company_name = get_company_name(ticker)


    # æ ¹æ®Kçº¿å‘¨æœŸè®¾ç½®åˆé€‚çš„æ—¶é—´èŒƒå›´
    # ä½¿ç”¨æ˜ç¡®çš„èµ·æ­¢æ—¥æœŸè€Œä¸æ˜¯range=maxï¼Œä»¥é¿å…Yahoo Finance APIçš„å·²çŸ¥bug
    # (ä½¿ç”¨range=maxå¯èƒ½ä¼šè¿”å›é”™è¯¯çš„æ•°æ®ç²’åº¦ï¼Œå¦‚è¯·æ±‚æ—¥çº¿å´è¿”å›å‘¨çº¿æ•°æ®)
    end_date = dt.datetime.now()
    start_date = end_date - dt.timedelta(days=365*20)  # æœ€å¤šè·å–20å¹´å†å²æ•°æ®

    # å°†æ—¥æœŸè½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
    period1 = int(start_date.timestamp())
    period2 = int(end_date.timestamp())

    if period_param == '1mo':
        interval_param = '1mo'
    elif period_param == '1wk':
        interval_param = '1wk'
    else:
        interval_param = '1d'

    url = f"https://query1.finance.yahoo.com/v8/finance/chart/{yahoo_ticker}?period1={period1}&period2={period2}&interval={interval_param}"
    print(f"è¯·æ±‚Yahoo Finance API: {url}")
    print(f"[API] ä½¿ç”¨Yahooæ ¼å¼: {yahoo_ticker} (åŸå§‹è¾“å…¥: {user_input_ticker})")

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        # å®‰å…¨åœ°è§£æJSONå“åº”
        try:
            yahoo_data = response.json()
        except ValueError as e:
            # å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼ˆå¦‚"Too Many Requests"æ–‡æœ¬ï¼‰
            error_text = response.text[:200] if response.text else "æ— å“åº”å†…å®¹"
            print(f"[ERROR] Yahoo Finance APIè¿”å›éJSONå“åº”: {error_text}")
            return jsonify({
                'error': 'Yahoo Finance APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç­‰å¾…3-5åˆ†é’Ÿåé‡è¯•',
                'details': f'APIè¿”å›: {error_text}',
                'suggestion': 'è¿™é€šå¸¸æ˜¯ç”±äºè¯·æ±‚é¢‘ç‡è¿‡é«˜å¯¼è‡´çš„ä¸´æ—¶é™åˆ¶'
            }), 503

        result = yahoo_data.get('chart', {}).get('result', [])
        if not result:
            return jsonify({'error': f"æ— æ³•ä»é›…è™è´¢ç»è·å–è‚¡ç¥¨ä»£ç ä¸º '{ticker}' çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®ã€‚"}), 404

        res = result[0]
        timestamps = res.get('timestamp', [])

        # å®‰å…¨åœ°è·å–quoteæ•°æ®ï¼Œé¿å…list index out of range
        indicators = res.get('indicators', {})
        quote_list = indicators.get('quote', [])

        if not quote_list:
            print(f"[ERROR] Yahoo Financeè¿”å›çš„æ•°æ®ä¸­æ²¡æœ‰quoteä¿¡æ¯")
            return jsonify({
                'error': f"æ— æ³•è§£æ '{ticker}' çš„è‚¡ä»·æ•°æ®",
                'details': 'Yahoo Financeè¿”å›çš„æ•°æ®æ ¼å¼ä¸å®Œæ•´'
            }), 500

        ohlc = quote_list[0]

        if not timestamps or not ohlc.get('open'):
             return jsonify({'error': f"è¿”å›çš„æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•è§£æ '{ticker}' çš„è‚¡ä»·ã€‚"}), 500
        
        # ä½¿ç”¨Pandas DataFrameè¿›è¡Œæ•°æ®åˆ†æ
        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': ohlc['open'],
            'high': ohlc['high'],
            'low': ohlc['low'],
            'close': ohlc['close'],
            'volume': ohlc['volume']
        })

        # ç§»é™¤ç©ºå€¼è¡Œï¼Œå¹¶ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
        df = df.dropna().copy()

        # åˆå§‹åŒ–åˆ†æç»“æœå®¹å™¨
        generated_annotations = []
        market_phases = []

        # --- V3.7: ä»æ•°æ®åº“è·å–æ‰€æœ‰æ³¨é‡Šï¼ˆåŒ…æ‹¬æ‰‹åŠ¨å’Œç®—æ³•æ³¨é‡Šï¼‰ ---
        existing_annotations = []
        try:
            db = get_db()
            cursor = db.cursor()
            # è·å–æ‰€æœ‰æœªåˆ é™¤çš„æ³¨é‡Š
            if IS_PRODUCTION: # V5.0
                cursor.execute("""
                    SELECT annotation_id, date, text, annotation_type, algorithm_type, is_favorite 
                    FROM annotations 
                    WHERE ticker = %s AND is_deleted = 0
                """, (ticker,))
            else:
                cursor.execute("""
                    SELECT annotation_id, date, text, annotation_type, algorithm_type, is_favorite 
                    FROM annotations 
                    WHERE ticker = ? AND is_deleted = 0
                """, (ticker,))
            annotation_rows = cursor.fetchall()
            existing_annotations = [
                {
                    'date': row['date'], 
                    'text': row['text'], 
                    'id': row['annotation_id'], 
                    'type': row['algorithm_type'] if row['annotation_type'] == 'algorithm' else row['annotation_type'],
                    'algorithm_type': row['algorithm_type'],
                    'is_favorite': bool(row['is_favorite']) if row['is_favorite'] is not None else False
                }
                for row in annotation_rows
            ]
        except Exception as e:
            print(f"Error fetching annotations from DB: {e}")
            existing_annotations = []
        finally:
            if 'db' in locals() and db:
                db.close()

        # åˆ†ç¦»æ‰‹åŠ¨æ³¨é‡Šå’Œç®—æ³•æ³¨é‡Šï¼ˆåŒ…æ‹¬AIåˆ†æï¼‰
        manual_annotations = [anno for anno in existing_annotations if anno['type'] == 'manual']
        existing_algorithm_annotations = [anno for anno in existing_annotations if anno['type'] in ['algorithm', 'price_volume', 'volume_stable_price', 'price_only', 'volume_only', 'ai_analysis']]

        # --- V1.2: å¯é…ç½®çš„åŠ¨æ€é˜ˆå€¼å¼‚å¸¸æ£€æµ‹ ---
        analysis_period = 60  # ä½¿ç”¨60ä¸ªå‘¨æœŸä½œä¸ºç»Ÿè®¡çª—å£
        
        if len(df) > analysis_period:
            # 1. è®¡ç®—ä»·æ ¼å’Œæˆäº¤é‡çš„åŠ¨æ€åŸºå‡†
            df['prev_close'] = df['close'].shift(1)
            df['price_change_pct'] = (df['close'] - df['prev_close']) / df['prev_close']
            
            df['price_change_std'] = df['price_change_pct'].rolling(window=analysis_period).std()
            df['volume_mean'] = df['volume'].rolling(window=analysis_period).mean()
            df['volume_std'] = df['volume'].rolling(window=analysis_period).std()

            # 2. å®šä¹‰å¼‚å¸¸æ¡ä»¶
            # ä»·é‡é½å‡çš„å¼‚å¸¸ä»·æ ¼
            is_abnormal_price_for_volume = df['price_change_pct'].abs() > (df['price_change_std'] * price_std_multiplier)
            # å•ç‹¬çš„ä»·æ ¼å¼‚å¸¸
            is_abnormal_price_only = df['price_change_pct'].abs() > (df['price_change_std'] * price_only_std_multiplier)
            # æˆäº¤é‡å¼‚å¸¸
            is_abnormal_volume = df['volume'] > (df['volume_mean'] + df['volume_std'] * volume_std_multiplier)
            # ä»·æ ¼ç¨³å®š
            is_stable_price = df['price_change_pct'].abs() < 0.01

            # 3. åº”ç”¨è§„åˆ™å¹¶ç”Ÿæˆæ ‡æ³¨
            # è§„åˆ™ä¸€ï¼šä»·é‡é½å‡/è·Œ
            abnormal_price_volume_days = df[is_abnormal_price_for_volume & is_abnormal_volume]
            for _, row in abnormal_price_volume_days.iterrows():
                change_type = "ä¸Šæ¶¨" if row['price_change_pct'] > 0 else "ä¸‹è·Œ"
                date_str = dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d')
                text = f'[ä»·é‡é½{change_type}] æ³¢åŠ¨: {row["price_change_pct"]:.2%}'
                
                # ä¿å­˜åˆ°æ•°æ®åº“å¹¶è·å–æ³¨é‡Šä¿¡æ¯
                annotation_result = save_algorithm_annotation(
                    ticker, date_str, text, 'price_volume',
                    {'price_std': price_std_multiplier, 'volume_std': volume_std_multiplier}
                )
                
                if annotation_result:
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…å†…å®¹ï¼ˆå¯èƒ½å·²è¢«ç”¨æˆ·ç¼–è¾‘ï¼‰
                    generated_annotations.append({
                        'date': date_str,
                        'text': annotation_result['text'],
                        'type': 'price_volume',
                        'id': annotation_result['id'],
                        'is_favorite': annotation_result.get('is_favorite', False)
                    })

            # è§„åˆ™äºŒï¼šæ”¾é‡æ»æ¶¨/è·Œ (æˆäº¤é‡å¼‚å¸¸ä½†ä»·æ ¼ç¨³å®š)
            abnormal_volume_stable_price_days = df[is_abnormal_volume & is_stable_price]
            for _, row in abnormal_volume_stable_price_days.iterrows():
                change_type = "ä¸Šæ¶¨" if row['price_change_pct'] > 0 else ("ä¸‹è·Œ" if row['price_change_pct'] < 0 else "å¹³ç›˜")
                date_str = dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d')
                text = f'[æ”¾é‡æ»{change_type}] æ³¢åŠ¨: {row["price_change_pct"]:.2%}'
                
                # ä¿å­˜åˆ°æ•°æ®åº“å¹¶è·å–æ³¨é‡Šä¿¡æ¯
                annotation_result = save_algorithm_annotation(
                    ticker, date_str, text, 'volume_stable_price',
                    {'volume_std': volume_std_multiplier}
                )
                
                if annotation_result:
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…å†…å®¹ï¼ˆå¯èƒ½å·²è¢«ç”¨æˆ·ç¼–è¾‘ï¼‰
                    generated_annotations.append({
                        'date': date_str,
                        'text': annotation_result['text'],
                        'type': 'volume_stable_price',
                        'id': annotation_result['id'],
                        'is_favorite': annotation_result.get('is_favorite', False)
                    })
            
            # è§„åˆ™ä¸‰ï¼šä»…ä»·æ ¼å¼‚å¸¸ (æˆäº¤é‡æœªæ˜¾è‘—æ”¾å¤§)
            # æˆ‘ä»¬è¦æ’é™¤æ‰å·²ç»è¢«è§„åˆ™ä¸€è¦†ç›–çš„æƒ…å†µ
            price_only_days = df[is_abnormal_price_only & ~is_abnormal_volume]
            for _, row in price_only_days.iterrows():
                change_type = "ä¸Šæ¶¨" if row['price_change_pct'] > 0 else "ä¸‹è·Œ"
                date_str = dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d')
                text = f'[ä»·å¼‚åŠ¨] {change_type} {row["price_change_pct"]:.2%}'
                
                # ä¿å­˜åˆ°æ•°æ®åº“å¹¶è·å–æ³¨é‡Šä¿¡æ¯
                annotation_result = save_algorithm_annotation(
                    ticker, date_str, text, 'price_only',
                    {'price_only_std': price_only_std_multiplier}
                )
                
                if annotation_result:
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…å†…å®¹ï¼ˆå¯èƒ½å·²è¢«ç”¨æˆ·ç¼–è¾‘ï¼‰
                    generated_annotations.append({
                        'date': date_str,
                        'text': annotation_result['text'],
                        'type': 'price_only',
                        'id': annotation_result['id'],
                        'is_favorite': annotation_result.get('is_favorite', False)
                    })

            # è§„åˆ™å››ï¼šä»…æˆäº¤é‡å¼‚å¸¸ (ä»·æ ¼æœªæ˜¾è‘—æ³¢åŠ¨)
            # V1.8 æ–°å¢ï¼šå½“æˆäº¤é‡å¼‚å¸¸ï¼Œä½†ä»·æ ¼æ³¢åŠ¨ä¸æ˜¾è‘—æ—¶
            is_abnormal_volume_only = df['volume'] > (df['volume_mean'] + df['volume_std'] * volume_only_std_multiplier)
            # æ’é™¤æ‰å·²ç»è¢«è§„åˆ™ä¸€å’Œè§„åˆ™äºŒè¦†ç›–çš„æƒ…å†µ
            volume_only_days = df[is_abnormal_volume_only & ~is_abnormal_price_for_volume & ~is_stable_price]
            for _, row in volume_only_days.iterrows():
                date_str = dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d')
                text = f'[é‡å¼‚åŠ¨]'
                
                # ä¿å­˜åˆ°æ•°æ®åº“å¹¶è·å–æ³¨é‡Šä¿¡æ¯
                annotation_result = save_algorithm_annotation(
                    ticker, date_str, text, 'volume_only',
                    {'volume_only_std': volume_only_std_multiplier}
                )
                
                if annotation_result:
                    # ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…å†…å®¹ï¼ˆå¯èƒ½å·²è¢«ç”¨æˆ·ç¼–è¾‘ï¼‰
                    generated_annotations.append({
                        'date': date_str,
                        'text': annotation_result['text'],
                        'type': 'volume_only',
                        'id': annotation_result['id'],
                        'is_favorite': annotation_result.get('is_favorite', False)
                    })

        # --- ZIGæŒ‡æ ‡å‡çº¿è®¡ç®— ---
        # ä»·æ ¼å‡çº¿
        df['ma5'] = df['close'].rolling(window=5).mean()
        df['ma25'] = df['close'].rolling(window=25).mean()
        df['ma50'] = df['close'].rolling(window=50).mean()
        
        # --- æ–°å¢ï¼šå¸¸ç”¨å‡çº¿è®¡ç®— ---
        df['ma5_new'] = df['close'].rolling(window=5).mean()  # 5æ—¥çº¿
        df['ma20'] = df['close'].rolling(window=20).mean()    # 20æ—¥çº¿  
        df['ma60_new'] = df['close'].rolling(window=60).mean() # 60æ—¥çº¿

        # æˆäº¤é‡å‡çº¿
        df['volume_ma5'] = df['volume'].rolling(window=5).mean()
        df['volume_ma25'] = df['volume'].rolling(window=25).mean()
        df['volume_ma50'] = df['volume'].rolling(window=50).mean()

        # è®¡ç®—ZIGæŒ‡æ ‡
        # ä»·æ ¼ZIG
        zig5 = calculate_zig(df['ma5'], short_term_zig_threshold)
        zig25 = calculate_zig(df['ma25'], medium_term_zig_threshold)
        zig50 = calculate_zig(df['ma50'], long_term_zig_threshold)
        
        # æˆäº¤é‡ZIG
        volume_zig5 = calculate_zig(df['volume_ma5'], volume_short_term_zig_threshold)
        volume_zig25 = calculate_zig(df['volume_ma25'], volume_medium_term_zig_threshold)
        volume_zig50 = calculate_zig(df['volume_ma50'], volume_long_term_zig_threshold)

        # --- V1.9: åŸºäºZIGæŒ‡æ ‡åˆ¤æ–­å¸‚åœºé˜¶æ®µ ---
        zig_map = {
            'zig5': zig5,
            'zig25': zig25,
            'zig50': zig50
        }
        selected_zig = zig_map.get(zig_phase_source, zig50) # é»˜è®¤ä½¿ç”¨zig50
        market_phases = calculate_phases_from_zig(selected_zig, df['timestamp'].tolist())

        # V2.0: åŸºäºæˆäº¤é‡ZIGåˆ¤æ–­æ”¾é‡/ç¼©é‡é˜¶æ®µ
        volume_zig_map = {
            'volume_zig5': volume_zig5,
            'volume_zig25': volume_zig25,
            'volume_zig50': volume_zig50
        }
        selected_volume_zig = volume_zig_map.get(volume_zig_phase_source, volume_zig50)
        volume_phases = calculate_phases_from_zig(selected_volume_zig, df['timestamp'].tolist())

        # V1.4 ä¿®å¤ï¼šå°†NaNæ›¿æ¢ä¸º0ï¼Œç¡®ä¿JSONæœ‰æ•ˆ
        if 'price_change_pct' not in df.columns:
            df['price_change_pct'] = 0.0
        df['price_change_pct'] = df['price_change_pct'].fillna(0)

        # å°†æ•´ä¸ªDataFrameä¸­çš„NaNæ›¿æ¢ä¸ºNoneï¼Œä»¥ä¾¿è¿›è¡Œæ­£ç¡®çš„JSONè½¬æ¢
        df.replace({np.nan: None}, inplace=True)

        k_data = []
        for index, row in df.iterrows():
            k_data.append([
                dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d'),
                row['open'],
                row['close'],
                row['low'],
                row['high'],
                row['volume'],
                row['price_change_pct'] * 100 if row['price_change_pct'] is not None else None
            ])
        
        # V3.7: åˆå¹¶æ‰€æœ‰æ³¨é‡Š - ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„æ³¨é‡Šï¼Œé¿å…é‡å¤
        all_annotations = manual_annotations + existing_algorithm_annotations + generated_annotations
        
        # V4.8.1: å¢å¼ºå»é‡å¤„ç† - åŒé‡å»é‡æœºåˆ¶
        # ç¬¬ä¸€æ­¥ï¼šåŸºäºæ³¨é‡ŠIDå»é‡ï¼Œé¿å…é‡å¤çš„è®°å½•
        seen_annotation_ids = set()
        id_deduped_annotations = []
        
        for anno in all_annotations:
            # åŸºäºæ³¨é‡ŠIDå»é‡ï¼Œæ¯ä¸ªæ³¨é‡Šéƒ½åº”è¯¥æœ‰å”¯ä¸€çš„ID
            annotation_id = anno.get('id')
            if annotation_id and annotation_id in seen_annotation_ids:
                continue
            
            if annotation_id:
                seen_annotation_ids.add(annotation_id)
            
            id_deduped_annotations.append(anno)
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºæ—¥æœŸ+ä¼˜å…ˆçº§å»é‡ï¼Œç¡®ä¿åŒä¸€æ—¥æœŸåªä¿ç•™æœ€æœ‰ä»·å€¼çš„è®°å½•
        date_priority_map = {}
        
        for anno in id_deduped_annotations:
            date = anno.get('date')
            if not date:
                continue
                
            # å®šä¹‰ä¼˜å…ˆçº§ï¼šAIåˆ†æ > æ‰‹åŠ¨ > ç®—æ³•
            # æ£€æŸ¥æ˜¯å¦ä¸ºAIåˆ†æè®°å½•ï¼ˆå¯èƒ½åœ¨typeæˆ–algorithm_typeå­—æ®µä¸­ï¼‰
            is_ai_analysis = (anno.get('algorithm_type') == 'ai_analysis' or 
                            anno.get('type') == 'ai_analysis')
            
            if is_ai_analysis:
                priority = 3  # AIåˆ†ææœ€é«˜ä¼˜å…ˆçº§
            elif anno.get('type') == 'manual':
                priority = 2  # æ‰‹åŠ¨æ³¨é‡Šä¼˜å…ˆçº§é«˜
            elif anno.get('type') in ['price_volume', 'volume_stable_price', 'price_only', 'volume_only']:
                priority = 1  # ç®—æ³•æ³¨é‡Šä¼˜å…ˆçº§æœ€ä½
            else:
                priority = 1  # é»˜è®¤ä¼˜å…ˆçº§
            
            # å¦‚æœè¯¥æ—¥æœŸè¿˜æ²¡æœ‰è®°å½•ï¼Œæˆ–è€…å½“å‰è®°å½•ä¼˜å…ˆçº§æ›´é«˜ï¼Œåˆ™æ›´æ–°
            if date not in date_priority_map or priority > date_priority_map[date]['priority']:
                date_priority_map[date] = {
                    'annotation': anno,
                    'priority': priority
                }
        
        # æå–æœ€ç»ˆçš„æ³¨é‡Šåˆ—è¡¨
        final_annotations = [info['annotation'] for info in date_priority_map.values()]
        
        print(f"æˆåŠŸè·å– {ticker} æ•°æ®ï¼Œå…± {len(k_data)} ä¸ªæ•°æ®ç‚¹ï¼Œ{len(final_annotations)} ä¸ªæ³¨é‡Š (æ‰‹åŠ¨:{len(manual_annotations)}, ç®—æ³•:{len(existing_algorithm_annotations)+len(generated_annotations)}, å»é‡å‰:{len(all_annotations)})")
        
        # å‡†å¤‡å‡çº¿æ•°æ®ï¼Œå¤„ç†NaNä¸ºNone
        ma5_data = [None if pd.isna(x) else x for x in df['ma5']]
        ma25_data = [None if pd.isna(x) else x for x in df['ma25']]
        ma50_data = [None if pd.isna(x) else x for x in df['ma50']]
        
        # æ–°å¢ï¼šå¸¸ç”¨å‡çº¿æ•°æ®
        ma5_new_data = [None if pd.isna(x) else x for x in df['ma5_new']]
        ma20_data = [None if pd.isna(x) else x for x in df['ma20']]
        ma60_new_data = [None if pd.isna(x) else x for x in df['ma60_new']]

        return jsonify({
            'ticker': ticker,
            'company_name': company_name,
            'data': k_data,
            'annotations': final_annotations, # V3.7: å°†åˆå¹¶åçš„æ‰€æœ‰æ ‡æ³¨æ•°æ®è¿”å›ç»™å‰ç«¯
            'market_phases': market_phases, # å°†å¸‚åœºé˜¶æ®µæ•°æ®è¿”å›ç»™å‰ç«¯
            'zig5': zig5,
            'zig25': zig25,
            'zig50': zig50,
            # V2.0 æ–°å¢ï¼šè¿”å›æˆäº¤é‡ZIGæ•°æ®
            'volume_zig5': volume_zig5,
            'volume_zig25': volume_zig25,
            'volume_zig50': volume_zig50,
            'volume_phases': volume_phases,
            # V2.1 æ¢å¤ï¼šè¿”å›å‡çº¿æ•°æ®
            'ma5': ma5_data,
            'ma25': ma25_data,
            'ma50': ma50_data,
            # æ–°å¢ï¼šå¸¸ç”¨å‡çº¿æ•°æ®
            'ma5_new': ma5_new_data,
            'ma20': ma20_data,
            'ma60_new': ma60_new_data
        })

    except requests.exceptions.HTTPError as http_err:
        print(f"[ERROR] Yahoo Finance HTTPError: {http_err}")
        return jsonify({'error': f"è¯·æ±‚é›…è™è´¢ç»APIæ—¶å‡ºé”™: {http_err}"}), 502
    except Exception as e:
        import traceback
        print(f"[ERROR] stock_dataå¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# --- V3.3: æ–°å¢ç»“æ„åŒ–åˆ†ææ•°æ®API ---
@app.route('/api/analysis_data')
@require_api_auth
def analysis_data():
    """
    æä¾›ç»“æ„åŒ–çš„è‚¡ä»·åˆ†ææ•°æ®API
    æ”¯æŒæ‰€æœ‰å¼‚å¸¸æ£€æµ‹å’ŒZIGæŒ‡æ ‡å‚æ•°çš„å®Œæ•´é…ç½®
    """
    import json
    import datetime as dt
    
    # --- è·å–æ‰€æœ‰å‚æ•° ---
    user_input_ticker = request.args.get('ticker', 'ONC')
    period_param = request.args.get('period', '1d')
    
    # --- æ™ºèƒ½è‚¡ç¥¨ä»£ç è¯†åˆ«ä¸è½¬æ¢ ---
    print(f"[ANALYSIS_API] ç”¨æˆ·è¾“å…¥: {user_input_ticker}")
    
    # Step 1: å°†ç”¨æˆ·è¾“å…¥æ ‡å‡†åŒ–ä¸ºå†…éƒ¨æ ¼å¼
    normalized_ticker, identification_type = normalize_ticker(user_input_ticker)
    
    if not normalized_ticker:
        smart_error_msg = generate_smart_error_message(user_input_ticker, identification_type)
        return jsonify({'error': smart_error_msg}), 400
    
    print(f"[ANALYSIS_API] æ ‡å‡†åŒ–ç»“æœ: {user_input_ticker} -> {normalized_ticker} (ç±»å‹: {identification_type})")
    
    # Step 2: ä¸ºYahoo APIå‡†å¤‡æ­£ç¡®æ ¼å¼
    yahoo_ticker = to_yahoo_format(normalized_ticker)
    
    # Step 3: è®¾ç½®å†…éƒ¨ä½¿ç”¨çš„tickerï¼ˆç”¨äºç¼“å­˜å’Œæ˜¾ç¤ºï¼‰
    ticker = normalized_ticker
    print(f"[ANALYSIS_API] æœ€ç»ˆä½¿ç”¨ - å†…éƒ¨æ ¼å¼: {ticker}, Yahooæ ¼å¼: {yahoo_ticker}")
    
    # å¼‚å¸¸æ£€æµ‹å‚æ•°
    price_std_multiplier = float(request.args.get('price_std', 1.8))
    volume_std_multiplier = float(request.args.get('volume_std', 1.8))
    price_only_std_multiplier = float(request.args.get('price_only_std', 2.5))
    volume_only_std_multiplier = float(request.args.get('volume_only_std', 3.0))

    # ZIGæŒ‡æ ‡å‚æ•°
    short_term_zig_threshold = float(request.args.get('short_term_zig', 10))
    medium_term_zig_threshold = float(request.args.get('medium_term_zig', 10))
    long_term_zig_threshold = float(request.args.get('long_term_zig', 25))
    zig_phase_source = request.args.get('zig_phase_source', 'zig50')

    # æˆäº¤é‡ZIGæŒ‡æ ‡å‚æ•°
    volume_short_term_zig_threshold = float(request.args.get('volume_short_term_zig', 10))
    volume_medium_term_zig_threshold = float(request.args.get('volume_medium_term_zig', 10))
    volume_long_term_zig_threshold = float(request.args.get('volume_long_term_zig', 10))
    volume_zig_phase_source = request.args.get('volume_zig_phase_source', 'volume_zig50')

    # è®°å½•ä½¿ç”¨çš„å‚æ•°
    used_parameters = {
        'ticker': ticker,
        'period': period_param,
        'price_std': price_std_multiplier,
        'volume_std': volume_std_multiplier,
        'price_only_std': price_only_std_multiplier,
        'volume_only_std': volume_only_std_multiplier,
        'short_term_zig': short_term_zig_threshold,
        'medium_term_zig': medium_term_zig_threshold,
        'long_term_zig': long_term_zig_threshold,
        'zig_phase_source': zig_phase_source,
        'volume_short_term_zig': volume_short_term_zig_threshold,
        'volume_medium_term_zig': volume_medium_term_zig_threshold,
        'volume_long_term_zig': volume_long_term_zig_threshold,
        'volume_zig_phase_source': volume_zig_phase_source
    }

    print(f"åˆ†æAPIè°ƒç”¨: {ticker}, å‚æ•°: {used_parameters}")

    try:
        # --- å¤ç”¨ç°æœ‰çš„æ•°æ®è·å–é€»è¾‘ ---
        # æ ¹æ®Kçº¿å‘¨æœŸè®¾ç½®åˆé€‚çš„æ—¶é—´èŒƒå›´
        if period_param == '1mo':
            range_param = '10y'
            interval_param = '1mo'
        elif period_param == '1wk':
            range_param = '10y'
            interval_param = '1wk'
        else:
            range_param = '10y'
            interval_param = '1d'

        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{yahoo_ticker}?range={range_param}&interval={interval_param}"
        print(f"[ANALYSIS_API] è¯·æ±‚Yahoo Finance API: {url}")
        print(f"[ANALYSIS_API] ä½¿ç”¨Yahooæ ¼å¼: {yahoo_ticker} (åŸå§‹è¾“å…¥: {user_input_ticker})")
        
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        
        yahoo_data = response.json()
        result = yahoo_data.get('chart', {}).get('result', [])
        if not result:
            return jsonify({'error': f"æ— æ³•è·å– '{ticker}' çš„æ•°æ®"}), 404

        res = result[0]
        timestamps = res.get('timestamp', [])
        ohlc = res.get('indicators', {}).get('quote', [{}])[0]

        if not timestamps or not ohlc.get('open'):
            return jsonify({'error': f"æ•°æ®æ ¼å¼ä¸å®Œæ•´"}), 500
        
        # æ•°æ®å¤„ç†
        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': ohlc['open'],
            'high': ohlc['high'],
            'low': ohlc['low'],
            'close': ohlc['close'],
            'volume': ohlc['volume']
        }).dropna().copy()

        # --- å¼‚å¸¸æ£€æµ‹åˆ†æ ---
        anomaly_results = {
            'price_volume_events': [],
            'volume_stable_price_events': [],
            'price_only_events': [],
            'volume_only_events': []
        }

        analysis_period = 60
        if len(df) > analysis_period:
            # è®¡ç®—åŠ¨æ€åŸºå‡†
            df['prev_close'] = df['close'].shift(1)
            df['price_change_pct'] = (df['close'] - df['prev_close']) / df['prev_close']
            df['price_change_std'] = df['price_change_pct'].rolling(window=analysis_period).std()
            df['volume_mean'] = df['volume'].rolling(window=analysis_period).mean()
            df['volume_std'] = df['volume'].rolling(window=analysis_period).std()

            # å¼‚å¸¸æ¡ä»¶åˆ¤æ–­
            is_abnormal_price_for_volume = df['price_change_pct'].abs() > (df['price_change_std'] * price_std_multiplier)
            is_abnormal_price_only = df['price_change_pct'].abs() > (df['price_change_std'] * price_only_std_multiplier)
            is_abnormal_volume = df['volume'] > (df['volume_mean'] + df['volume_std'] * volume_std_multiplier)
            is_stable_price = df['price_change_pct'].abs() < 0.01
            is_abnormal_volume_only = df['volume'] > (df['volume_mean'] + df['volume_std'] * volume_only_std_multiplier)

            # è§„åˆ™ä¸€ï¼šä»·é‡é½å‡/è·Œ
            for _, row in df[is_abnormal_price_for_volume & is_abnormal_volume].iterrows():
                anomaly_results['price_volume_events'].append({
                    'date': dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d'),
                    'price_change_pct': round(row['price_change_pct'] * 100, 2),
                    'volume': int(row['volume']),
                    'close_price': round(row['close'], 2),
                    'type': 'ä¸Šæ¶¨' if row['price_change_pct'] > 0 else 'ä¸‹è·Œ'
                })

            # è§„åˆ™äºŒï¼šæ”¾é‡æ»æ¶¨/è·Œ
            for _, row in df[is_abnormal_volume & is_stable_price].iterrows():
                anomaly_results['volume_stable_price_events'].append({
                    'date': dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d'),
                    'price_change_pct': round(row['price_change_pct'] * 100, 2),
                    'volume': int(row['volume']),
                    'close_price': round(row['close'], 2),
                    'type': 'æ»æ¶¨' if row['price_change_pct'] >= 0 else 'æ»è·Œ'
                })

            # è§„åˆ™ä¸‰ï¼šä»…ä»·æ ¼å¼‚å¸¸
            for _, row in df[is_abnormal_price_only & ~is_abnormal_volume].iterrows():
                anomaly_results['price_only_events'].append({
                    'date': dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d'),
                    'price_change_pct': round(row['price_change_pct'] * 100, 2),
                    'volume': int(row['volume']),
                    'close_price': round(row['close'], 2),
                    'type': 'ä¸Šæ¶¨' if row['price_change_pct'] > 0 else 'ä¸‹è·Œ'
                })

            # è§„åˆ™å››ï¼šä»…æˆäº¤é‡å¼‚å¸¸
            for _, row in df[is_abnormal_volume_only & ~is_abnormal_price_for_volume & ~is_stable_price].iterrows():
                anomaly_results['volume_only_events'].append({
                    'date': dt.datetime.fromtimestamp(row['timestamp']).strftime('%Y-%m-%d'),
                    'price_change_pct': round(row['price_change_pct'] * 100, 2),
                    'volume': int(row['volume']),
                    'close_price': round(row['close'], 2),
                    'type': 'æ”¾é‡'
                })

        # --- ZIGæŒ‡æ ‡åˆ†æ ---
        # è®¡ç®—å‡çº¿
        df['ma5'] = df['close'].rolling(window=5).mean()
        df['ma25'] = df['close'].rolling(window=25).mean()
        df['ma50'] = df['close'].rolling(window=50).mean()
        df['volume_ma5'] = df['volume'].rolling(window=5).mean()
        df['volume_ma25'] = df['volume'].rolling(window=25).mean()
        df['volume_ma50'] = df['volume'].rolling(window=50).mean()

        # è®¡ç®—ZIGæŒ‡æ ‡
        zig5 = calculate_zig(df['ma5'], short_term_zig_threshold)
        zig25 = calculate_zig(df['ma25'], medium_term_zig_threshold)
        zig50 = calculate_zig(df['ma50'], long_term_zig_threshold)
        volume_zig5 = calculate_zig(df['volume_ma5'], volume_short_term_zig_threshold)
        volume_zig25 = calculate_zig(df['volume_ma25'], volume_medium_term_zig_threshold)
        volume_zig50 = calculate_zig(df['volume_ma50'], volume_long_term_zig_threshold)

        # æå–ZIGè½¬æŠ˜ç‚¹
        def extract_zig_points(zig_series, timestamps, zig_name):
            points = []
            for i, value in enumerate(zig_series):
                if value is not None:
                    points.append({
                        'date': dt.datetime.fromtimestamp(timestamps[i]).strftime('%Y-%m-%d'),
                        'value': round(value, 2),
                        'index': i,
                        'zig_type': zig_name
                    })
            return points

        zig_analysis = {
            'zig5_points': extract_zig_points(zig5, df['timestamp'].tolist(), 'short_term'),
            'zig25_points': extract_zig_points(zig25, df['timestamp'].tolist(), 'medium_term'),
            'zig50_points': extract_zig_points(zig50, df['timestamp'].tolist(), 'long_term'),
            'volume_zig5_points': extract_zig_points(volume_zig5, df['timestamp'].tolist(), 'volume_short_term'),
            'volume_zig25_points': extract_zig_points(volume_zig25, df['timestamp'].tolist(), 'volume_medium_term'),
            'volume_zig50_points': extract_zig_points(volume_zig50, df['timestamp'].tolist(), 'volume_long_term')
        }

        # --- å¸‚åœºé˜¶æ®µåˆ†æ ---
        zig_map = {'zig5': zig5, 'zig25': zig25, 'zig50': zig50}
        selected_zig = zig_map.get(zig_phase_source, zig50)
        market_phases = calculate_phases_from_zig(selected_zig, df['timestamp'].tolist())

        volume_zig_map = {'volume_zig5': volume_zig5, 'volume_zig25': volume_zig25, 'volume_zig50': volume_zig50}
        selected_volume_zig = volume_zig_map.get(volume_zig_phase_source, volume_zig50)
        volume_phases = calculate_phases_from_zig(selected_volume_zig, df['timestamp'].tolist())

        # --- ç»Ÿè®¡ä¿¡æ¯ ---
        statistics = {
            'total_anomalies': sum(len(events) for events in anomaly_results.values()),
            'price_volume_count': len(anomaly_results['price_volume_events']),
            'volume_stable_price_count': len(anomaly_results['volume_stable_price_events']),
            'price_only_count': len(anomaly_results['price_only_events']),
            'volume_only_count': len(anomaly_results['volume_only_events']),
            'market_phases_count': len(market_phases),
            'volume_phases_count': len(volume_phases),
            'zig5_points_count': len(zig_analysis['zig5_points']),
            'zig25_points_count': len(zig_analysis['zig25_points']),
            'zig50_points_count': len(zig_analysis['zig50_points']),
            'data_points': len(df)
        }

        # --- è¿”å›ç»“æ„åŒ–æ•°æ® ---
        return jsonify({
            'meta': {
                'ticker': ticker,
                'period': period_param,
                'analysis_timestamp': dt.datetime.now().isoformat(),
                'parameters': used_parameters
            },
            'anomaly_analysis': anomaly_results,
            'zig_analysis': zig_analysis,
            'market_phases': market_phases,
            'volume_phases': volume_phases,
            'statistics': statistics
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

# --- è‚¡ç¥¨åå•ç¼“å­˜ç®¡ç†API ---
@app.route('/api/stock-search', methods=['GET'])
@require_api_auth
def search_stocks():
    """
    è‚¡ç¥¨æœç´¢API - æ”¯æŒè‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æœç´¢
    å‚æ•°ï¼š
    - q: æœç´¢å…³é”®è¯ï¼ˆè‚¡ç¥¨ä»£ç æˆ–å…¬å¸åç§°ï¼‰
    - limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼ˆé»˜è®¤10ï¼‰
    """
    try:
        query = request.args.get('q', '').strip()
        limit = int(request.args.get('limit', 10))
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'
            }), 400
        
        print(f"[SEARCH_API] æœç´¢å…³é”®è¯: {query}")
        
        # å°è¯•æ™ºèƒ½è¯†åˆ«
        normalized_ticker, identification_type = normalize_ticker(query)
        
        results = []
        
        # å¦‚æœæ˜¯æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if normalized_ticker and identification_type not in ['company_name_not_found', 'search_error', 'invalid']:
            company_name = get_company_name(normalized_ticker)
            
            results.append({
                'ticker': normalized_ticker,
                'company_name': company_name,
                'match_type': 'exact_code',
                'display_name': f"{company_name} ({to_display_format(normalized_ticker)})"
            })
        
        # æœç´¢å…¬å¸åç§°åŒ¹é…
        db = get_db()
        cursor = db.cursor()
        
        # æ¨¡ç³Šæœç´¢å…¬å¸åç§°
        cursor.execute("""
            SELECT ticker, company_name, source 
            FROM company_names 
            WHERE company_name LIKE %s 
            ORDER BY 
                CASE WHEN company_name = %s THEN 1 ELSE 2 END,
                LENGTH(company_name) ASC
            LIMIT %s
        """, (f"%{query}%", query, limit))
        
        name_matches = cursor.fetchall()
        
        for match in name_matches:
            ticker = match['ticker']
            company_name = match['company_name']
            source = match['source']
            
            # é¿å…é‡å¤æ·»åŠ ï¼ˆå¦‚æœå‰é¢å·²ç»é€šè¿‡ä»£ç è¯†åˆ«æ·»åŠ äº†ï¼‰
            if not any(r['ticker'] == ticker for r in results):
                results.append({
                    'ticker': ticker,
                    'company_name': company_name,
                    'match_type': 'company_name',
                    'display_name': f"{company_name} ({to_display_format(ticker)})",
                    'source': source
                })
        
        # å¦‚æœæŸ¥è¯¢æ˜¯çº¯æ•°å­—ï¼Œä¹Ÿæœç´¢åŒ…å«è¯¥æ•°å­—çš„è‚¡ç¥¨ä»£ç 
        if query.isdigit():
            cursor.execute("""
                SELECT ticker, company_name, source 
                FROM company_names 
                WHERE ticker LIKE %s 
                ORDER BY LENGTH(ticker) ASC
                LIMIT %s
            """, (f"%{query}%", limit))
            
            code_matches = cursor.fetchall()
            
            for match in code_matches:
                ticker = match['ticker']
                company_name = match['company_name']
                source = match['source']
                
                # é¿å…é‡å¤æ·»åŠ 
                if not any(r['ticker'] == ticker for r in results):
                    results.append({
                        'ticker': ticker,
                        'company_name': company_name,
                        'match_type': 'partial_code',
                        'display_name': f"{company_name} ({to_display_format(ticker)})",
                        'source': source
                    })
        
        cursor.close()
        db.close()
        
        # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
        results = results[:limit]
        
        return jsonify({
            'success': True,
            'query': query,
            'results': results,
            'total': len(results)
        })
        
    except Exception as e:
        print(f"[ERROR] è‚¡ç¥¨æœç´¢APIå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# --- V4.5: æ–°å¢è¶‹åŠ¿åŒºé—´åˆ†æAPI ---
@app.route('/api/trend-analysis')
@require_api_auth
def trend_analysis():
    """
    è¶‹åŠ¿åŒºé—´åˆ†æAPI - æä¾›è‚¡ç¥¨æŒ‡å®šæ—¶é—´æ®µå†…çš„ä¸Šæ¶¨/ä¸‹è·ŒåŒºé—´åˆ†æ
    æ”¯æŒæ—¶é—´æ®µç­›é€‰ã€å¼‚å¸¸ç‚¹å…³è”å’Œå½“å‰åŒºé—´çŠ¶æ€åˆ¤æ–­
    """
    import json
    import datetime as dt
    
    try:
        # è·å–è¯·æ±‚å‚æ•°
        user_input_ticker = request.args.get('ticker', 'UNH')
        period_param = request.args.get('period', 'all')  # æ”¯æŒ 1y, 2y, 3y, 5y, æˆ– all
        
        # æ™ºèƒ½è‚¡ç¥¨ä»£ç è¯†åˆ«ä¸è½¬æ¢
        print(f"[TREND_API] ç”¨æˆ·è¾“å…¥: {user_input_ticker}, æ—¶é—´æ®µ: {period_param}")
        
        normalized_ticker, identification_type = normalize_ticker(user_input_ticker)
        if not normalized_ticker:
            smart_error_msg = generate_smart_error_message(user_input_ticker, identification_type)
            return jsonify({'error': smart_error_msg}), 400
        
        yahoo_ticker = to_yahoo_format(normalized_ticker)
        ticker = normalized_ticker
        
        print(f"[TREND_API] æ ‡å‡†åŒ–ç»“æœ: {user_input_ticker} -> {ticker}")
        
        # è®¾ç½®æ•°æ®è·å–èŒƒå›´
        # ä½¿ç”¨æ˜ç¡®çš„èµ·æ­¢æ—¥æœŸè€Œä¸æ˜¯rangeå‚æ•°ï¼Œä»¥é¿å…Yahoo Finance APIçš„å·²çŸ¥bug
        # (ä½¿ç”¨rangeå‚æ•°å¯èƒ½ä¼šè¿”å›é”™è¯¯çš„æ•°æ®ç²’åº¦ï¼Œå¦‚è¯·æ±‚æ—¥çº¿å´è¿”å›å‘¨çº¿æ•°æ®)
        end_date = dt.datetime.now()

        if period_param.lower() == 'all':
            # period=ALL ä½¿ç”¨20å¹´æ•°æ®ï¼ˆä¸ä¸»APIä¿æŒä¸€è‡´ï¼‰
            start_date = end_date - dt.timedelta(days=365*20)
        elif period_param.endswith('y'):
            # è§£æå¹´ä»½å‚æ•°ï¼Œå¦‚ 5y, 10y, 20y
            years = int(period_param.replace('y', ''))
            start_date = end_date - dt.timedelta(days=365*years)
        else:
            # é»˜è®¤3å¹´
            start_date = end_date - dt.timedelta(days=365*3)

        # å°†æ—¥æœŸè½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
        period1 = int(start_date.timestamp())
        period2 = int(end_date.timestamp())

        # è·å–è‚¡ä»·æ•°æ®
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{yahoo_ticker}?period1={period1}&period2={period2}&interval=1d"
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        
        yahoo_data = response.json()
        result = yahoo_data.get('chart', {}).get('result', [])
        if not result:
            return jsonify({'error': f"æ— æ³•è·å– '{ticker}' çš„è‚¡ä»·æ•°æ®"}), 404
        
        res = result[0]
        timestamps = res.get('timestamp', [])
        ohlc = res.get('indicators', {}).get('quote', [{}])[0]
        
        if not timestamps or not ohlc.get('open'):
            return jsonify({'error': f"æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ— æ³•è§£æ '{ticker}' çš„è‚¡ä»·"}), 500
        
        # åˆ›å»ºDataFrameè¿›è¡Œåˆ†æ
        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': ohlc['open'],
            'high': ohlc['high'],
            'low': ohlc['low'],
            'close': ohlc['close'],
            'volume': ohlc['volume']
        })
        
        # æ¸…ç†æ•°æ®
        df = df.dropna().copy()
        if df.empty:
            return jsonify({'error': 'æ•°æ®æ¸…ç†åä¸ºç©º'}), 500
        
        # è®¡ç®—50æ—¥ç§»åŠ¨å¹³å‡çº¿ï¼ˆä¸å‰ç«¯ä¿æŒä¸€è‡´ï¼‰
        df['ma50'] = df['close'].rolling(window=50).mean()
        
        # è®¡ç®—ZIGæŒ‡æ ‡ï¼ˆä»URLå‚æ•°è·å–ï¼Œé»˜è®¤å€¼ä¸º25ï¼ŒåŸºäºMA50å‡çº¿ï¼‰
        zig_threshold = float(request.args.get('long_term_zig', 25))
        print(f"[TREND_API] ä½¿ç”¨ZIGé˜ˆå€¼: {zig_threshold}%ï¼ŒåŸºäºMA50å‡çº¿")
        zig_series = calculate_zig(df['ma50'], zig_threshold)
        
        # è®¡ç®—è¶‹åŠ¿åŒºé—´
        market_phases = calculate_phases_from_zig(zig_series, df['timestamp'].tolist())
        
        # å¦‚æœæŒ‡å®šäº†æ—¶é—´æ®µï¼Œè¿›è¡Œç­›é€‰
        if period_param != 'all':
            years = int(period_param.replace('y', ''))
            cutoff_date = dt.datetime.now() - dt.timedelta(days=365 * years)
            cutoff_timestamp = cutoff_date.timestamp()
            
            # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µå†…çš„åŒºé—´
            filtered_phases = []
            for phase in market_phases:
                phase_start = dt.datetime.strptime(phase['start_date'], '%Y-%m-%d').timestamp()
                if phase_start >= cutoff_timestamp:
                    filtered_phases.append(phase)
            market_phases = filtered_phases
        
        # è·å–æ³¨é‡Šæ•°æ®
        db = get_db()
        cursor = db.cursor()
        if IS_PRODUCTION: # V5.0
            cursor.execute('''
                SELECT date, text, annotation_type, algorithm_type
                FROM annotations 
                WHERE ticker = %s AND is_deleted = 0
                ORDER BY date ASC
            ''', (ticker,))
        else:
            cursor.execute('''
                SELECT date, text, annotation_type, algorithm_type
                FROM annotations 
                WHERE ticker = ? AND is_deleted = 0
                ORDER BY date ASC
            ''', (ticker,))
        
        annotations_data = cursor.fetchall()
        cursor.close()
        db.close()
        
        # ä¸ºæ¯ä¸ªåŒºé—´å…³è”å¼‚å¸¸ç‚¹
        trend_periods = []
        for phase in market_phases:
            # è½¬æ¢phaseç±»å‹ä¸ºä¸­æ–‡
            phase_chinese = "ä¸Šæ¶¨åŒºé—´" if phase['phase'] == 'Uptrend' else "ä¸‹è·ŒåŒºé—´"
            
            # è®¡ç®—åŒºé—´æŒç»­å¤©æ•°
            start_date = dt.datetime.strptime(phase['start_date'], '%Y-%m-%d')
            end_date = dt.datetime.strptime(phase['end_date'], '%Y-%m-%d')
            duration_days = (end_date - start_date).days
            
            # è·å–èµ·å§‹å’Œç»“æŸæ—¥æœŸçš„è‚¡ä»·
            start_timestamp = start_date.timestamp()
            end_timestamp = end_date.timestamp()
            
            # ä»DataFrameä¸­æŸ¥æ‰¾æœ€æ¥è¿‘çš„è‚¡ä»·æ•°æ®
            start_price = None
            end_price = None
            price_change_pct = None
            
            # æŸ¥æ‰¾èµ·å§‹æ—¥æœŸçš„è‚¡ä»·ï¼ˆæ”¶ç›˜ä»·ï¼‰
            start_idx = (df['timestamp'] - start_timestamp).abs().idxmin()
            if not pd.isna(df.loc[start_idx, 'close']):
                start_price = round(df.loc[start_idx, 'close'], 2)
            
            # æŸ¥æ‰¾ç»“æŸæ—¥æœŸçš„è‚¡ä»·ï¼ˆæ”¶ç›˜ä»·ï¼‰
            end_idx = (df['timestamp'] - end_timestamp).abs().idxmin()
            if not pd.isna(df.loc[end_idx, 'close']):
                end_price = round(df.loc[end_idx, 'close'], 2)
            
            # è®¡ç®—æ¶¨è·Œå¹…
            if start_price and end_price:
                price_change_pct = round(((end_price - start_price) / start_price) * 100, 2)
            
            # ç­›é€‰è¯¥åŒºé—´å†…çš„å¼‚å¸¸ç‚¹
            period_anomalies = []
            for annotation in annotations_data:
                annotation_date = dt.datetime.strptime(annotation['date'], '%Y-%m-%d')
                if start_date <= annotation_date <= end_date:
                    anomaly_type = annotation['algorithm_type'] if annotation['annotation_type'] == 'algorithm' else 'manual'
                    period_anomalies.append({
                        'date': annotation['date'],
                        'text': annotation['text'],
                        'type': anomaly_type
                    })
            
            trend_periods.append({
                'phase': phase_chinese,
                'start_date': phase['start_date'],
                'end_date': phase['end_date'],
                'duration_days': duration_days,
                'start_price': start_price,
                'end_price': end_price,
                'price_change_pct': price_change_pct,
                'anomalies': period_anomalies
            })
        
        # V5.7: ä¼˜åŒ–æœ€åä¸€ä¸ªåŒºé—´ï¼Œæ¶ˆé™¤æ—¶é—´ç¼ºå£é—®é¢˜
        if trend_periods:
            last_period = trend_periods[-1]
            last_end_date = dt.datetime.strptime(last_period['end_date'], '%Y-%m-%d')
            current_date = dt.datetime.now()
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—¶é—´ç¼ºå£ï¼ˆè¶…è¿‡30å¤©ï¼‰
            days_gap = (current_date - last_end_date).days
            if days_gap > 30:
                print(f"[TREND_API] å‘ç°æ—¶é—´ç¼ºå£: {days_gap}å¤©ï¼Œä¼˜åŒ–æœ€ååŒºé—´")
                
                # è·å–æœ€æ–°è‚¡ä»·æ•°æ®ï¼ˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ä»·ï¼‰
                latest_price = None
                latest_idx = df['close'].last_valid_index()
                if latest_idx is not None:
                    latest_price = round(df.loc[latest_idx, 'close'], 2)
                
                if latest_price and last_period['start_price']:
                    # é‡æ–°è®¡ç®—åŸºäºæœ€æ–°è‚¡ä»·çš„æ¶¨è·Œå¹…
                    new_price_change_pct = round(((latest_price - last_period['start_price']) / last_period['start_price']) * 100, 2)
                    
                    # é‡æ–°è®¡ç®—æŒç»­å¤©æ•°
                    start_date = dt.datetime.strptime(last_period['start_date'], '%Y-%m-%d')
                    new_duration_days = (current_date - start_date).days
                    
                    # æ›´æ–°æœ€åä¸€ä¸ªåŒºé—´
                    trend_periods[-1].update({
                        'end_date': current_date.strftime('%Y-%m-%d'),
                        'duration_days': new_duration_days,
                        'end_price': latest_price,
                        'price_change_pct': new_price_change_pct
                    })
                    
                    # V5.7.1: é‡æ–°ç­›é€‰æ‰©å±•åŒºé—´å†…çš„anomaliesï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
                    extended_end_date = current_date
                    start_date_obj = dt.datetime.strptime(last_period['start_date'], '%Y-%m-%d')
                    extended_anomalies = []
                    
                    print(f"[TREND_API] é‡æ–°ç­›é€‰anomalies: {last_period['start_date']} -> {extended_end_date.strftime('%Y-%m-%d')}")
                    
                    for annotation in annotations_data:
                        annotation_date = dt.datetime.strptime(annotation['date'], '%Y-%m-%d')
                        if start_date_obj <= annotation_date <= extended_end_date:
                            anomaly_type = annotation['algorithm_type'] if annotation['annotation_type'] == 'algorithm' else 'manual'
                            extended_anomalies.append({
                                'date': annotation['date'],
                                'text': annotation['text'],
                                'type': anomaly_type
                            })
                    
                    # æ›´æ–°anomalies
                    trend_periods[-1]['anomalies'] = extended_anomalies
                    print(f"[TREND_API] anomaliesæ›´æ–°: {len(last_period['anomalies'])} -> {len(extended_anomalies)} ä¸ªäº‹ä»¶")
                    
                    print(f"[TREND_API] åŒºé—´ä¼˜åŒ–å®Œæˆ: {last_period['start_date']} -> {current_date.strftime('%Y-%m-%d')}, æ¶¨è·Œå¹…: {new_price_change_pct}%")
        
        # V5.7.x: å¦‚èµ·å§‹å­˜åœ¨ç©ºç™½åŒºé—´ï¼Œå‘å‰å»¶ä¼¸é¦–ä¸ªåŒºé—´ä»¥å¡«è¡¥ç¼ºå£
        if trend_periods:
            earliest_ts = df['timestamp'].min()
            earliest_date = dt.datetime.fromtimestamp(earliest_ts).strftime('%Y-%m-%d')
            
            first_period = trend_periods[0]
            first_start_obj = dt.datetime.strptime(first_period['start_date'], '%Y-%m-%d')
            earliest_date_obj = dt.datetime.strptime(earliest_date, '%Y-%m-%d')
            
            if earliest_date_obj < first_start_obj:
                print(f"[TREND_API] å‘ç°èµ·å§‹ç¼ºå£: {first_period['start_date']} ä¹‹å‰å­˜åœ¨æ•°æ®ï¼Œå‘å‰å»¶ä¼¸è‡³ {earliest_date}")
                
                end_date_obj = dt.datetime.strptime(first_period['end_date'], '%Y-%m-%d')
                first_period['start_date'] = earliest_date
                first_period['duration_days'] = (end_date_obj - earliest_date_obj).days
                
                # é‡æ–°è®¡ç®—èµ·å§‹ä»·æ ¼ä¸æ¶¨è·Œå¹…
                start_idx = (df['timestamp'] - earliest_date_obj.timestamp()).abs().idxmin()
                if not pd.isna(df.loc[start_idx, 'close']):
                    first_period['start_price'] = round(df.loc[start_idx, 'close'], 2)
                
                if first_period.get('start_price') and first_period.get('end_price'):
                    first_period['price_change_pct'] = round(((first_period['end_price'] - first_period['start_price']) / first_period['start_price']) * 100, 2)
                
                # é‡æ–°ç­›é€‰å»¶ä¼¸ååŒºé—´å†…çš„å¼‚å¸¸ç‚¹
                extended_anomalies = []
                for annotation in annotations_data:
                    annotation_date = dt.datetime.strptime(annotation['date'], '%Y-%m-%d')
                    if earliest_date_obj <= annotation_date <= end_date_obj:
                        anomaly_type = annotation['algorithm_type'] if annotation['annotation_type'] == 'algorithm' else 'manual'
                        extended_anomalies.append({
                            'date': annotation['date'],
                            'text': annotation['text'],
                            'type': anomaly_type
                        })
                
                first_period['anomalies'] = extended_anomalies
                print(f"[TREND_API] èµ·å§‹åŒºé—´å»¶ä¼¸å®Œæˆ: {earliest_date} -> {first_period['end_date']}, æ¶¨è·Œå¹…: {first_period.get('price_change_pct')}")
        
        # V5.7: åŸºäºä¼˜åŒ–åçš„è¶‹åŠ¿åŒºé—´åˆ¤æ–­å½“å‰è‚¡ä»·çŠ¶æ€
        current_trend = None
        if trend_periods:
            latest_period = trend_periods[-1]
            current_start = dt.datetime.strptime(latest_period['start_date'], '%Y-%m-%d')
            current_duration = (dt.datetime.now() - current_start).days
            
            current_trend = {
                'phase': latest_period['phase'],
                'start_date': latest_period['start_date'],
                'duration_days': current_duration,
                'current_price': latest_period['end_price'],
                'start_price': latest_period['start_price'],
                'price_change_pct': latest_period['price_change_pct']
            }
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        uptrend_periods = [p for p in trend_periods if p['phase'] == 'ä¸Šæ¶¨åŒºé—´']
        downtrend_periods = [p for p in trend_periods if p['phase'] == 'ä¸‹è·ŒåŒºé—´']
        
        total_uptrend_days = sum(p['duration_days'] for p in uptrend_periods)
        total_downtrend_days = sum(p['duration_days'] for p in downtrend_periods)
        total_anomalies = sum(len(p['anomalies']) for p in trend_periods)
        
        # è®¾ç½®åˆ†ææ—¶é—´æ®µæè¿°
        if period_param == 'all':
            period_desc = f"å…¨éƒ¨å†å²æ•°æ®"
        else:
            years = int(period_param.replace('y', ''))
            start_date = dt.datetime.now() - dt.timedelta(days=365 * years)
            period_desc = f"{start_date.strftime('%Y-%m-%d')} è‡³ {dt.datetime.now().strftime('%Y-%m-%d')}"
        
        return jsonify({
            'success': True,
            'ticker': user_input_ticker,
            'analysis_period': period_desc,
            'zig_threshold_used': zig_threshold,
            'current_trend': current_trend,
            'trend_periods': trend_periods,
            'statistics': {
                'total_uptrend_days': total_uptrend_days,
                'total_downtrend_days': total_downtrend_days,
                'uptrend_periods': len(uptrend_periods),
                'downtrend_periods': len(downtrend_periods),
                'total_anomalies': total_anomalies
            }
        })
        
    except Exception as e:
        print(f"[ERROR] è¶‹åŠ¿åˆ†æAPIå¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/api/stock-list/update', methods=['POST'])
@require_api_auth
def update_stock_list():
    """æ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨åå•ç¼“å­˜API"""
    try:
        print("[API] æ”¶åˆ°è‚¡ç¥¨åå•æ›´æ–°è¯·æ±‚")
        
        # æ›´æ–°è‚¡ç¥¨åå•ç¼“å­˜
        updated_count = update_stock_list_cache()
        
        return jsonify({
            'success': True,
            'message': f'è‚¡ç¥¨åå•ç¼“å­˜æ›´æ–°å®Œæˆï¼Œå¤„ç†äº† {updated_count} æ¡è®°å½•',
            'updated_count': updated_count
        })
        
    except Exception as e:
        print(f"[ERROR] è‚¡ç¥¨åå•æ›´æ–°APIå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/stock-list/stats', methods=['GET'])
@require_api_auth
def get_stock_list_stats():
    """è·å–è‚¡ç¥¨åå•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯API"""
    try:
        db = get_db()
        cursor = db.cursor()
        
        # ç»Ÿè®¡æ€»æ•°
        cursor.execute("SELECT COUNT(*) as total FROM company_names")
        total = cursor.fetchone()['total']
        
        # æŒ‰æ¥æºç»Ÿè®¡
        cursor.execute("""
            SELECT source, COUNT(*) as count 
            FROM company_names 
            GROUP BY source 
            ORDER BY count DESC
        """)
        source_stats = [{'source': row['source'], 'count': row['count']} for row in cursor.fetchall()]
        
        # æŒ‰äº¤æ˜“æ‰€ç»Ÿè®¡ï¼ˆé€šè¿‡tickeråç¼€åˆ¤æ–­ï¼‰
        cursor.execute("""
            SELECT 
                CASE 
                    WHEN ticker LIKE '%.SZ' THEN 'SZ'
                    WHEN ticker LIKE '%.SH' THEN 'SH'
                    WHEN ticker LIKE '%.hk' OR ticker LIKE '%.HK' THEN 'HK'
                    ELSE 'OTHER'
                END as exchange,
                COUNT(*) as count
            FROM company_names
            GROUP BY exchange
            ORDER BY count DESC
        """)
        exchange_stats = [{'exchange': row['exchange'], 'count': row['count']} for row in cursor.fetchall()]
        
        cursor.close()
        db.close()
        
        return jsonify({
            'success': True,
            'stats': {
                'total_companies': total,
                'by_source': source_stats,
                'by_exchange': exchange_stats
            }
        })
        
    except Exception as e:
        print(f"[ERROR] è·å–è‚¡ç¥¨åå•ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# --- V4.8.1: æ–°å¢ç‰¹å®šæ—¥æœŸè‚¡ä»·æ³¢åŠ¨è·å–APIï¼Œç”¨äºæ‰‹åŠ¨æ³¨é‡ŠAIåˆ†æ ---
@app.route('/api/stock_data/<string:ticker>/<string:date>')
@require_api_auth
def get_stock_data_for_date(ticker, date):
    """è·å–ç‰¹å®šæ—¥æœŸçš„è‚¡ä»·æ³¢åŠ¨æ•°æ®ï¼Œç”¨äºAIåˆ†æç©ºå†…å®¹çš„æ‰‹åŠ¨æ³¨é‡Š"""
    print(f"[API] è·å–è‚¡ä»·æ³¢åŠ¨æ•°æ®: {ticker} on {date}")
    
    try:
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        normalized_ticker, identification_type = normalize_ticker(ticker)
        if not normalized_ticker:
            smart_error_msg = generate_smart_error_message(ticker, identification_type)
            return jsonify({'error': smart_error_msg}), 400
        
        # ä¸ºYahoo APIå‡†å¤‡æ­£ç¡®æ ¼å¼
        yahoo_ticker = to_yahoo_format(normalized_ticker)
        print(f"[API] ä½¿ç”¨Yahooæ ¼å¼: {yahoo_ticker}")
        
        # è·å–æœ€è¿‘30å¤©çš„æ•°æ®ä»¥ç¡®ä¿åŒ…å«ç›®æ ‡æ—¥æœŸ
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{yahoo_ticker}?range=1mo&interval=1d"
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        
        yahoo_data = response.json()
        result = yahoo_data.get('chart', {}).get('result', [])
        if not result:
            return jsonify({'error': f"æ— æ³•è·å– {ticker} çš„è‚¡ä»·æ•°æ®"}), 404
        
        res = result[0]
        timestamps = res.get('timestamp', [])
        ohlc = res.get('indicators', {}).get('quote', [{}])[0]
        
        if not timestamps or not ohlc.get('open'):
            return jsonify({'error': f"è‚¡ä»·æ•°æ®æ ¼å¼ä¸å®Œæ•´"}), 500
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': ohlc['open'],
            'high': ohlc['high'],
            'low': ohlc['low'],
            'close': ohlc['close'],
            'volume': ohlc.get('volume', [0] * len(timestamps))
        })
        
        # è¿‡æ»¤æ— æ•ˆæ•°æ®
        df = df.dropna()
        if df.empty:
            return jsonify({'error': f"æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ä»·æ•°æ®"}), 404
        
        # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸ
        df['date'] = pd.to_datetime(df['timestamp'], unit='s').dt.strftime('%Y-%m-%d')
        
        # æŸ¥æ‰¾ç›®æ ‡æ—¥æœŸçš„æ•°æ®
        target_data = df[df['date'] == date]
        if target_data.empty:
            return jsonify({'error': f"æœªæ‰¾åˆ° {date} çš„è‚¡ä»·æ•°æ®"}), 404
        
        row = target_data.iloc[0]
        
        # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆéœ€è¦å‰ä¸€äº¤æ˜“æ—¥æ•°æ®ï¼‰
        prev_data = df[df['date'] < date].tail(1)
        if not prev_data.empty:
            prev_close = prev_data.iloc[0]['close']
            change_pct = ((row['close'] - prev_close) / prev_close) * 100
        else:
            change_pct = 0
        
        # è®¡ç®—å½“æ—¥æŒ¯å¹…
        amplitude = ((row['high'] - row['low']) / row['low']) * 100
        
        # è·å–å…¬å¸åç§°
        company_name = get_company_name(normalized_ticker)
        
        # æ ¼å¼åŒ–æ•°æ®ä¸ºAIå‹å¥½çš„æ–‡æœ¬
        volatility_text = f"""è‚¡ä»·æ³¢åŠ¨æƒ…å†µï¼š
å¼€ç›˜ä»·ï¼š{row['open']:.2f}
æœ€é«˜ä»·ï¼š{row['high']:.2f}
æœ€ä½ä»·ï¼š{row['low']:.2f}
æ”¶ç›˜ä»·ï¼š{row['close']:.2f}
æˆäº¤é‡ï¼š{int(row['volume']):,}
æ¶¨è·Œå¹…ï¼š{change_pct:+.2f}%
å½“æ—¥æŒ¯å¹…ï¼š{amplitude:.2f}%"""
        
        # æ ¼å¼åŒ–ç”¨æˆ·æ³¨é‡Šæ–‡æœ¬ - ä¸ºæ–°å»ºæ³¨é‡Šæä¾›è§„èŒƒåŒ–å†…å®¹
        formatted_annotation_text = f"""{company_name} {normalized_ticker} è‚¡ä»·å¼‚åŠ¨æ—¶ç‚¹ï¼š{date}
è‚¡ä»·æ³¢åŠ¨{change_pct:+.2f}%"""
        
        return jsonify({
            'success': True,
            'ticker': normalized_ticker,
            'company_name': company_name,
            'date': date,
            'volatility_text': volatility_text,
            'formatted_annotation_text': formatted_annotation_text,
            'data': {
                'open': float(row['open']),
                'high': float(row['high']),
                'low': float(row['low']),
                'close': float(row['close']),
                'volume': int(row['volume']),
                'change_pct': round(change_pct, 2),
                'amplitude': round(amplitude, 2)
            }
        })
        
    except Exception as e:
        print(f"[ERROR] è·å–è‚¡ä»·æ³¢åŠ¨æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'è·å–è‚¡ä»·æ•°æ®å¤±è´¥: {str(e)}'
        }), 500


# ===== æ ¸å¿ƒAPIè·¯ç”± =====

@app.route('/api/annotations/<string:ticker>', methods=['GET'])
@require_api_auth
def get_annotations(ticker):
    """è·å–æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰æ³¨é‡Šæ•°æ®"""
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æ™ºèƒ½æŸ¥è¯¢é€‚é…
        if IS_PRODUCTION: # V5.0
            query = """
                SELECT annotation_id, ticker, date, text, annotation_type, algorithm_type, 
                       algorithm_params, original_text, ai_analysis, is_favorite, created_at, updated_at
                FROM annotations 
                WHERE ticker = %s AND is_deleted = 0
                ORDER BY date DESC
            """
        else:
            query = """
                SELECT annotation_id, ticker, date, text, annotation_type, algorithm_type, 
                       algorithm_params, original_text, ai_analysis, is_favorite, created_at, updated_at
                FROM annotations 
                WHERE ticker = ? AND is_deleted = 0
                ORDER BY date DESC
            """
        
        cursor.execute(query, (ticker,))
        rows = cursor.fetchall()
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        annotations = []
        for row in rows:
            annotations.append({
                'annotation_id': row['annotation_id'],
                'ticker': row['ticker'], 
                'date': row['date'],
                'text': row['text'],
                'annotation_type': row['annotation_type'],
                'algorithm_type': row['algorithm_type'],
                'algorithm_params': row['algorithm_params'],
                'original_text': row['original_text'],
                'ai_analysis': row['ai_analysis'],
                'is_favorite': bool(row['is_favorite']) if row['is_favorite'] is not None else False,
                'created_at': str(row['created_at']),
                'updated_at': str(row['updated_at'])
            })
        
        cursor.close()
        db.close()
        
        return jsonify(annotations)
        
    except Exception as e:
        print(f"[ERROR] è·å–æ³¨é‡Šå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/stock/<string:ticker>', methods=['GET'])  
def get_stock_basic(ticker):
    """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
    try:
        # è·å–å…¬å¸åç§°
        company_name = get_company_name(ticker)
        
        return jsonify({
            'ticker': ticker,
            'company_name': company_name,
            'status': 'success'
        })
        
    except Exception as e:
        print(f"[ERROR] è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

# --- æ•°æ®åº“è¿ç§»ç®¡ç†API (ä»…é™ç®¡ç†å‘˜) ---
@app.route('/admin/execute-migration', methods=['POST'])
@require_api_auth
def execute_migration():
    """
    å®‰å…¨çš„æ•°æ®åº“è¿ç§»æ‰§è¡Œæ¥å£
    åªå…è®¸æ‰§è¡Œé¢„å®šä¹‰çš„INSERTè¯­å¥
    """
    try:
        data = request.get_json()
        
        if not data or 'migration_type' not in data:
            return jsonify({'error': 'ç¼ºå°‘è¿ç§»ç±»å‹å‚æ•°'}), 400
        
        migration_type = data['migration_type']
        
        # åªå…è®¸ç‰¹å®šçš„è¿ç§»ç±»å‹
        if migration_type not in ['test_a_stocks', 'full_a_stocks']:
            return jsonify({'error': 'ä¸æ”¯æŒçš„è¿ç§»ç±»å‹'}), 400
        
        # å®‰å…¨æ£€æŸ¥ï¼šåªå…è®¸INSERT OR IGNOREè¯­å¥
        if migration_type == 'test_a_stocks':
            # æ‰§è¡Œæµ‹è¯•è¿ç§»ï¼ˆ5æ¡æ•°æ®ï¼‰
            test_data = [
                ('000001.SZ', 'å¹³å®‰é“¶è¡Œ', '2025-07-13 10:18:18', 'stock_list_local', '2025-07-13 10:18:18'),
                ('000002.SZ', 'ä¸‡ ç§‘ï¼¡', '2025-07-13 10:18:18', 'stock_list_local', '2025-07-13 10:18:18'),
                ('603688.SH', 'çŸ³è‹±è‚¡ä»½', '2025-07-13 10:18:18', 'stock_list_local', '2025-07-13 10:18:18'),
                ('000858.SZ', 'äº” ç²® æ¶²', '2025-07-13 10:18:18', 'stock_list_local', '2025-07-13 10:18:18'),
                ('600036.SH', 'æ‹›å•†é“¶è¡Œ', '2025-07-13 10:18:18', 'stock_list_local', '2025-07-13 10:18:18')
            ]
            
            db = get_db()
            cursor = db.cursor()
            success_count = 0
            
            for ticker, company_name, created_at, source, last_updated in test_data:
                try:
                    if USE_POSTGRESQL:
                        # PostgreSQLä½¿ç”¨ON CONFLICT DO NOTHING
                        db_execute(cursor, '''
                            INSERT INTO company_names (ticker, company_name, created_at, source, last_updated) 
                            VALUES (%s, %s, %s, %s, %s) 
                            ON CONFLICT (ticker) DO NOTHING
                        ''', (ticker, company_name, created_at, source, last_updated))
                    else:
                        # SQLiteä½¿ç”¨INSERT OR IGNORE
                        db_execute(cursor, '''
                            INSERT OR IGNORE INTO company_names (ticker, company_name, created_at, source, last_updated) 
                            VALUES (?, ?, ?, ?, ?)
                        ''', (ticker, company_name, created_at, source, last_updated))
                    success_count += 1
                    print(f"[MIGRATION] æˆåŠŸæ·»åŠ : {ticker} - {company_name}")
                except Exception as e:
                    print(f"[MIGRATION] æ‰§è¡Œå¤±è´¥: {ticker} - {company_name} é”™è¯¯: {str(e)}")
            
            db.commit()  # æäº¤äº‹åŠ¡ï¼
            cursor.close()
            db.close()
            
            return jsonify({
                'success': True,
                'type': 'test_migration',
                'executed': success_count,
                'total': len(test_data),
                'message': f'æµ‹è¯•è¿ç§»å®Œæˆï¼ŒæˆåŠŸæ‰§è¡Œ {success_count}/{len(test_data)} æ¡è¯­å¥'
            })
            
        elif migration_type == 'full_a_stocks':
            # æ‰§è¡Œå®Œæ•´çš„Aè‚¡æ•°æ®è¿ç§»
            migration_file = 'migration_a_stocks.sql'
            if not os.path.exists(migration_file):
                return jsonify({'error': 'migration_a_stocks.sqlæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨'}), 400
            
            db = get_db()
            cursor = db.cursor()
            success_count = 0
            error_count = 0
            
            print(f"[MIGRATION] å¼€å§‹æ‰§è¡Œå®Œæ•´Aè‚¡æ•°æ®è¿ç§»...")
            
            try:
                with open(migration_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # è§£æSQLæ–‡ä»¶ä¸­çš„INSERTè¯­å¥ï¼ˆæ”¯æŒINSERT OR IGNOREï¼‰
                import re
                insert_pattern = r"INSERT(?:\s+OR\s+IGNORE)?\s+INTO\s+company_names\s+\([^)]+\)\s+VALUES\s+\(([^)]+)\);"
                matches = re.findall(insert_pattern, content, re.IGNORECASE)
                
                total_records = len(matches)
                print(f"[MIGRATION] æ‰¾åˆ°{total_records}æ¡å¾…è¿ç§»æ•°æ®")
                
                for match in matches:
                    try:
                        # è§£æVALUESä¸­çš„æ•°æ®
                        values_str = match.strip()
                        # ç®€å•è§£æï¼Œå‡è®¾æ ¼å¼æ˜¯: 'ticker', 'company_name', 'created_at', 'source', 'last_updated'
                        values_parts = [v.strip().strip("'\"") for v in values_str.split(',')]
                        
                        if len(values_parts) >= 5:
                            ticker, company_name, created_at, source, last_updated = values_parts[:5]
                            
                            if USE_POSTGRESQL:
                                # PostgreSQLä½¿ç”¨ON CONFLICT DO NOTHING
                                db_execute(cursor, '''
                                    INSERT INTO company_names (ticker, company_name, created_at, source, last_updated) 
                                    VALUES (%s, %s, %s, %s, %s) 
                                    ON CONFLICT (ticker) DO NOTHING
                                ''', (ticker, company_name, created_at, source, last_updated))
                            else:
                                # SQLiteä½¿ç”¨INSERT OR IGNORE
                                db_execute(cursor, '''
                                    INSERT OR IGNORE INTO company_names (ticker, company_name, created_at, source, last_updated) 
                                    VALUES (?, ?, ?, ?, ?)
                                ''', (ticker, company_name, created_at, source, last_updated))
                            
                            success_count += 1
                            if success_count % 100 == 0:
                                print(f"[MIGRATION] å·²å¤„ç† {success_count}/{total_records} æ¡è®°å½•...")
                        else:
                            error_count += 1
                    except Exception as e:
                        error_count += 1
                        print(f"[MIGRATION] æ‰§è¡Œå¤±è´¥: {str(e)}")
                
                db.commit()  # æäº¤äº‹åŠ¡
                cursor.close()
                db.close()
                
                print(f"[MIGRATION] å®Œæ•´è¿ç§»å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
                
                return jsonify({
                    'success': True,
                    'type': 'full_migration',
                    'executed': success_count,
                    'total': total_records,
                    'errors': error_count,
                    'message': f'å®Œæ•´è¿ç§»å®Œæˆï¼ŒæˆåŠŸæ‰§è¡Œ {success_count}/{total_records} æ¡è®°å½•ï¼Œé”™è¯¯ {error_count} æ¡'
                })
                
            except Exception as e:
                db.rollback()
                cursor.close()
                db.close()
                print(f"[MIGRATION] è¿ç§»è¿‡ç¨‹å‡ºé”™: {str(e)}")
                return jsonify({'error': f'è¿ç§»è¿‡ç¨‹å‡ºé”™: {str(e)}'}), 500
            
    except Exception as e:
        print(f"[ERROR] è¿ç§»æ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify({'error': f'è¿ç§»æ‰§è¡Œå¤±è´¥: {str(e)}'}), 500

@app.route('/admin/migration-status', methods=['GET'])
@require_api_auth  
def migration_status():
    """
    æ£€æŸ¥è¿ç§»çŠ¶æ€ - æŸ¥çœ‹Aè‚¡æ•°æ®æ˜¯å¦å­˜åœ¨
    """
    try:
        db = get_db()
        cursor = db.cursor()
        
        # æ£€æŸ¥å…³é”®Aè‚¡ç®€ç§°æ˜¯å¦å­˜åœ¨
        test_companies = ['å¹³å®‰é“¶è¡Œ', 'çŸ³è‹±è‚¡ä»½', 'ä¸‡ ç§‘ï¼¡', 'æ‹›å•†é“¶è¡Œ', 'äº” ç²® æ¶²']
        results = {}
        
        for company in test_companies:
            db_execute(cursor, "SELECT ticker FROM company_names WHERE company_name = %s", (company,))
            result = cursor.fetchone()
            results[company] = result['ticker'] if result else None
        
        # ç»Ÿè®¡Aè‚¡æ•°æ®æ€»æ•°
        db_execute(cursor, "SELECT COUNT(*) as count FROM company_names WHERE source = 'stock_list_local'")
        total_count = cursor.fetchone()['count']
        
        cursor.close()
        db.close()
        
        return jsonify({
            'success': True,
            'test_companies': results,
            'total_a_stocks': total_count,
            'migration_needed': total_count < 1000  # å¦‚æœå°‘äº1000æ¡è¯´æ˜éœ€è¦è¿ç§»
        })
        
    except Exception as e:
        print(f"[ERROR] çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
        return jsonify({'error': f'çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
